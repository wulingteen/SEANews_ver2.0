import base64
import hashlib
import json
import os
import uuid
import time
import secrets
import threading
from datetime import datetime, timedelta
from mimetypes import guess_type
from pathlib import Path
from typing import Any, Dict, Iterator, List, Optional, Union, Literal, Set

import dotenv
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, Field

from agno.agent import Agent
from agno.media import Image
from agno.run.agent import RunEvent
from agno.run.team import TeamRunEvent
from agno.team import Team
from agno.models.openai import OpenAIChat
from agno.models.openai.responses import OpenAIResponses

# Lazy import for RagStore to avoid startup failures
# from rag_store import RagStore
from tag_store import get_doc_tags, load_tag_store, set_custom_tags, set_doc_tags, clear_all_tags
from email_service import send_email_with_attachment, generate_news_report_html
from excel_service import (
    generate_news_excel, 
    generate_batch_news_excel, 
    cleanup_old_exports,
    batch_translate_titles,
    extract_country_from_content
)
from news_store import news_store


# Robust .env loader to avoid parser crashes on some environments.
def _safe_load_env() -> None:
    env_path = os.path.join(os.path.dirname(__file__), "..", ".env")
    try:
        dotenv.load_dotenv(env_path, override=True)
    except Exception:
        # Fallback to system environment variables.
        return


_safe_load_env()

# === å•Ÿå‹•è¨ºæ–·æ—¥èªŒ ===
print("=" * 60)
print("[å•Ÿå‹•] SEANews æ‡‰ç”¨æ­£åœ¨åˆå§‹åŒ–...")
print(f"[ç’°å¢ƒ] Python è·¯å¾‘: {os.getcwd()}")
print(f"[ç’°å¢ƒ] PYTHONPATH: {os.getenv('PYTHONPATH', 'NOT SET')}")
print(f"[ç’°å¢ƒ] PORT: {os.getenv('PORT', 'NOT SET')}")
print(f"[ç’°å¢ƒ] OPENAI_API_KEY: {'å·²è¨­ç½® âœ“' if os.getenv('OPENAI_API_KEY') else 'æœªè¨­ç½® âœ— (å°‡å°è‡´å•Ÿå‹•å¤±æ•—!)'}")
print(f"[ç’°å¢ƒ] OPENAI_MODEL: {os.getenv('OPENAI_MODEL', 'NOT SET')}")
print(f"[ç’°å¢ƒ] APP_USERNAME: {'å·²è¨­ç½® âœ“' if os.getenv('APP_USERNAME') else 'æœªè¨­ç½® âœ—'}")
print(f"[ç’°å¢ƒ] APP_SECRET_KEY: {'å·²è¨­ç½® âœ“' if os.getenv('APP_SECRET_KEY') else 'æœªè¨­ç½® âœ—'}")
print("=" * 60)

# ä¿¡ä»»çš„æ±å—äºæ–°èä¾†æº
TRUSTED_NEWS_SOURCES = [
    {"name": "VietJo", "domain": "viet-jo.com", "region": "Vietnam"},
    {"name": "Cafef", "domain": "cafef.vn", "region": "Vietnam"},
    {"name": "VNExpress", "domain": "vnexpress.net", "region": "Vietnam"},
    {"name": "Vietnam Finance", "domain": "vietnamfinance.vn", "region": "Vietnam"},
    {"name": "Vietnam Investment Review", "domain": "vir.com.vn", "region": "Vietnam"},
    {"name": "Vietnambiz", "domain": "vietnambiz.vn", "region": "Vietnam"},
    {"name": "Tap Chi Tai chinh", "domain": "tapchikinhtetaichinh.vn", "region": "Vietnam"},
    {"name": "Bangkok Post", "domain": "bangkokpost.com", "region": "Thailand"},
    {"name": "Techsauce", "domain": "techsauce.co", "region": "Thailand"},
    {"name": "Fintech Singapore", "domain": "fintechnews.sg", "region": "Singapore"},
    {"name": "Fintech Philippines", "domain": "fintechnews.ph", "region": "Philippines"},
    {"name": "Khmer Times", "domain": "khmertimeskh.com", "region": "Cambodia"},
    {"name": "æŸ¬ä¸­æ™‚å ±", "domain": "cc-times.com", "region": "Cambodia"},
    {"name": "The Phnom Penh Post", "domain": "phnompenhpost.com", "region": "Cambodia"},
    {"name": "Deal Street Asia", "domain": "dealstreetasia.com", "region": "Southeast Asia"},
    {"name": "Tech in Asia", "domain": "techinasia.com", "region": "Southeast Asia"},
    {"name": "Nikkei Asia", "domain": "asia.nikkei.com", "region": "Southeast Asia"},
    {"name": "Heaptalk", "domain": "heaptalk.com", "region": "Southeast Asia"},
]

TEAM_INSTRUCTIONS = [
    "ä½ æ˜¯æ±å—äºæ–°èè¼¿æƒ…åˆ†æåŠ©ç†ï¼Œå°ˆç²¾æ–¼æ±å—äºå€åŸŸæ–°èæœå°‹ã€ç¿»è­¯èˆ‡æ·±åº¦åˆ†æã€‚",
    "ä½ å¯ä»¥èˆ‡ä½¿ç”¨è€…è‡ªç„¶å°è©±ï¼Œå”åŠ©æœå°‹ã€æ‘˜è¦ã€ç¿»è­¯æ±å—äºå„åœ‹çš„æ–°èè³‡è¨Šã€‚",
    "",
    "ã€é‡è¦ã€‘æ ¹æ“šä½¿ç”¨è€…æ„åœ–é¸æ“‡å›è¦†æ¨¡å¼ï¼š",
    "1. å•å€™/é–’èŠï¼ˆå¦‚ hi, hello, ä½ å¥½ï¼‰â†’ ä½¿ç”¨ã€Œç°¡å–®æ¨¡å¼ã€",
    "2. éœ€è¦æ–°èæ–‡ä»¶åˆ†æï¼ˆå¦‚ æ‘˜è¦ã€ç¿»è­¯ï¼‰â†’ ä½¿ç”¨ã€Œå®Œæ•´æ¨¡å¼ã€ä¸¦å§”æ´¾ RAG Agentï¼ˆæ–‡ä»¶æª¢ç´¢ï¼‰",
    "3. éœ€è¦æœå°‹æ–°è/å¸‚å ´è³‡è¨Šï¼ˆæ–°èã€ç”¢æ¥­å‹•æ…‹ã€æ”¿ç­–è®ŠåŒ–ï¼‰â†’ ä½¿ç”¨ã€Œæ–°èæœå°‹æ¨¡å¼ã€ä¸¦å§”æ´¾ Deep Research Agentï¼Œå¿…é ˆä½¿ç”¨ web_search å·¥å…·åŸ·è¡Œæ·±åº¦æœå°‹ï¼Œå„ªå…ˆæœå°‹ä¿¡ä»»æ–°èä¾†æºã€‚",
    "4. ä½¿ç”¨è€…æä¾›æˆªåœ–/ç…§ç‰‡/å½±åƒ â†’ å§”æ´¾ Vision Agent è®€åœ–èˆ‡ OCRï¼Œä¸¦å›å‚³é‡é»èˆ‡æ–‡å­—å…§å®¹ã€‚",
    "è‹¥æœ¬æ¬¡ä»»å‹™åŒ…å« OCR æ–‡å­—ï¼Œè«‹åœ¨ summary.output ç”¢å‡ºè©²æ–‡ä»¶çš„æ‘˜è¦ã€‚",
    "",
    "ã€æ–°èæœå°‹æ¨¡å¼ - è¼¸å‡ºæ ¼å¼è¦æ±‚ã€‘",
    "ç•¶åŸ·è¡Œæ–°èæœå°‹æ™‚ï¼Œassistant.content å¿…é ˆåŒ…å« Markdown æ ¼å¼çš„æ–°èåˆ—è¡¨ï¼Œæ¯å‰‡æ–°èåŒ…å«ï¼š",
    "- æ–°èæ¨™é¡Œï¼ˆä½¿ç”¨ ### æ¨™è¨˜ï¼‰",
    "- ç™¼å¸ƒæ™‚é–“ï¼ˆæ ¼å¼ï¼šYYYY-MM-DD æˆ– YYYYå¹´MMæœˆDDæ—¥ï¼‰",
    "- æ–°èæ‘˜è¦ï¼ˆ1-2 æ®µæ–‡å­—ï¼‰",
    "- æ–°èä¾†æºé€£çµï¼ˆå®Œæ•´ URLï¼‰",
    "",
    "ç¯„ä¾‹æ ¼å¼ï¼š",
    "### è¶Šå—å¤®è¡Œå®£å¸ƒé™æ¯ 0.5 å€‹ç™¾åˆ†é»",
    "ç™¼å¸ƒæ™‚é–“ï¼š2025-12-28",
    "è¶Šå—åœ‹å®¶éŠ€è¡Œï¼ˆSBVï¼‰ä»Šæ—¥å®£å¸ƒå°‡åŸºæº–åˆ©ç‡ä¸‹èª¿ 0.5 å€‹ç™¾åˆ†é»è‡³ 4.5%ï¼Œé€™æ˜¯ä»Šå¹´ç¬¬ä¸‰æ¬¡é™æ¯ã€‚æ­¤èˆ‰æ—¨åœ¨åˆºæ¿€ç¶“æ¿Ÿæˆé•·ä¸¦æ”¯æŒä¼æ¥­èè³‡ã€‚",
    "https://vnexpress.net/economy/example-url",
    "",
    "### æ³°åœ‹é€šéæ–°æŠ•è³‡ä¿ƒé€²æ³•æ¡ˆ",
    "ç™¼å¸ƒæ™‚é–“ï¼š2025-12-27",
    "æ³°åœ‹å…§é–£æ‰¹å‡†æ–°çš„æŠ•è³‡ä¿ƒé€²æ³•æ¡ˆï¼Œç‚ºå¤–åœ‹æŠ•è³‡è€…æä¾›æœ€é«˜ 8 å¹´çš„ç¨…æ”¶å„ªæƒ ã€‚é‡é»ç”¢æ¥­åŒ…æ‹¬é›»å‹•è»Šã€æ•¸ä½ç¶“æ¿Ÿå’Œç”Ÿç‰©ç§‘æŠ€ã€‚",
    "https://bangkokpost.com/business/example-url",
    "",
    "ã€ç°¡å–®æ¨¡å¼ã€‘åƒ…å¡«å…… assistant.contentï¼Œå…¶ä»–æ¬„ä½å¿…é ˆç‚ºç©ºæˆ–ç©ºé™£åˆ—ï¼š",
    '{"assistant": {"content": "ä½ å¥½ï¼æˆ‘æ˜¯æ±å—äºæ–°èè¼¿æƒ…åˆ†æåŠ©ç†ï¼Œå¯ä»¥å”åŠ©æ‚¨æœå°‹ã€æ‘˜è¦ã€ç¿»è­¯æ±å—äºå„åœ‹æ–°èã€‚æœ‰ä»€éº¼æˆ‘èƒ½å¹«å¿™çš„å—ï¼Ÿ", "bullets": []}, "summary": {"output": "", "borrower": null, "metrics": [], "risks": []}, "translation": {"output": "", "clauses": []}, "memo": {"output": "", "sections": [], "recommendation": "", "conditions": ""}, "routing": []}',
    "",
    "ã€å®Œæ•´æ¨¡å¼ã€‘å¡«å……ç›¸é—œ artifacts ä¸¦è¨˜éŒ„ routing æ­¥é©Ÿ",
    "",
    "ã€JSON æ ¼å¼è¦æ±‚ã€‘",
    "- å›è¦†å¿…é ˆæ˜¯åš´æ ¼ JSONï¼Œä¸å¯è¼¸å‡º Markdown code fence æˆ–å¤šé¤˜èªªæ˜",
    "- summary.output èˆ‡ memo.output ç”¨ç¹é«”ä¸­æ–‡",
    "- summary.output ä¸­ä¸è¦ä½¿ç”¨åœ‹å®¶åç¨±ï¼ˆå¦‚ ##è¶Šå—ã€##æ³°åœ‹ã€##Vietnam ç­‰ï¼‰ä½œç‚ºæ¨™é¡Œï¼Œç›´æ¥æè¿°å…§å®¹å³å¯",
    "- translation.output èˆ‡ translation.clauses[].translated ç”¨è‹±æ–‡",
    "- summary.source_doc_id èˆ‡ translation.source_doc_id å¿…é ˆå¡«å…¥ä¾†æºæ–‡ä»¶çš„ idï¼ˆè¦‹æ–‡ä»¶æ¸…å–®ä¸­çš„ idï¼‰",
    "- è‹¥ä¾†æºç‚ºå¤šä»½æ–‡ä»¶ï¼Œå¯ä½¿ç”¨ summary.source_doc_ids / translation.source_doc_ids é™£åˆ—",
    "- summary.risks[].level åƒ…èƒ½æ˜¯ Highã€Mediumã€Low",
    "- routing ç”±ç³»çµ±å¡«å¯«ï¼Œè«‹å›å‚³ç©ºé™£åˆ— []",
]

EXPECTED_OUTPUT = """
ç°¡å–®æ¨¡å¼ç¯„ä¾‹ï¼ˆå•å€™/é–’èŠï¼‰ï¼š
{
  "assistant": { "content": "ä½ å¥½ï¼æˆ‘æ˜¯æ±å—äºæ–°èè¼¿æƒ…åˆ†æåŠ©ç†ï¼Œå¯ä»¥å”åŠ©æ‚¨æœå°‹ã€æ‘˜è¦ã€ç¿»è­¯æ±å—äºå„åœ‹æ–°èã€‚æœ‰ä»€éº¼æˆ‘èƒ½å¹«å¿™çš„å—ï¼Ÿ", "bullets": [] },
  "summary": { "output": "", "borrower": null, "metrics": [], "risks": [], "source_doc_id": "" },
  "translation": { "output": "", "clauses": [], "source_doc_id": "" },
  "memo": { "output": "", "sections": [], "recommendation": "", "conditions": "" },
  "routing": []
}

å®Œæ•´æ¨¡å¼ç¯„ä¾‹ï¼ˆæ–°èæœå°‹/åˆ†æï¼‰ï¼š
{
  "assistant": { "content": "å·²å®Œæˆæ–°èæœå°‹èˆ‡åˆ†æ", "bullets": ["æœå°‹æ±å—äºæ–°èä¾†æº", "æå–é—œéµè³‡è¨Š", "ç”Ÿæˆæ‘˜è¦åˆ†æ"] },
  "summary": {
    "output": "## æ–°èæ‘˜è¦\næ‰¾åˆ° 5 ç¯‡ç›¸é—œæ–°è...",
    "source_doc_id": "news-1",
    "borrower": { "name": "æ–°èæ¨™é¡Œ", "description": "ä¾†æºèˆ‡æ‘˜è¦", "rating": "" },
    "metrics": [{ "label": "ç™¼å¸ƒæ™‚é–“", "value": "2025-12-29", "delta": "" }],
    "risks": [{ "label": "è³‡è¨Šå¯ä¿¡åº¦", "level": "Low" }]
  },
  "translation": { "output": "", "clauses": [], "source_doc_id": "" },
  "memo": { "output": "", "sections": [], "recommendation": "", "conditions": "" },
  "routing": []
}
""".strip()

RAG_AGENT_INSTRUCTIONS = [
    "ä½ æ˜¯æ–‡ä»¶æª¢ç´¢èˆ‡è§£æå°ˆå“¡ï¼Œè² è²¬ä½¿ç”¨ RAG æœå°‹ä¸Šå‚³æ–‡ä»¶ã€‚",
    "æ”¶åˆ°ä»»å‹™å¾Œï¼Œå…ˆä½¿ç”¨ search_knowledge_base å·¥å…·æª¢ç´¢ç›¸é—œç‰‡æ®µã€‚",
    "å›è¦†è«‹åˆ—å‡ºèˆ‡éœ€æ±‚æœ€ç›¸é—œçš„æ‘˜éŒ„èˆ‡é ç¢¼/æ®µè½è³‡è¨Šï¼Œé¿å…ç·¨é€ ã€‚",
    "è‹¥æ‰¾ä¸åˆ°ç›¸é—œå…§å®¹ï¼Œè«‹æ˜ç¢ºå›è¦†ã€æœªæ‰¾åˆ°ç›¸é—œæ®µè½ã€ã€‚",
]

# Lazy initialization to avoid startup errors if dependencies are missing
_rag_store = None

def get_rag_store():
    """
    Lazy initialization of RagStore to prevent startup failures.
    Returns a dummy object if initialization fails (e.g., missing pypdf or OPENAI_API_KEY).
    """
    global _rag_store
    if _rag_store is None:
        try:
            # Lazy import to avoid import-time errors
            from rag_store import RagStore
            _rag_store = RagStore()
            print("âœ“ RagStore initialized successfully")
        except Exception as e:
            print(f"âš  Warning: RagStore initialization failed: {e}")
            print("  RAG features will be disabled. App will continue without RAG support.")
            # Return a dummy object that prevents crashes
            class DummyRagStore:
                docs = {}
                def index_inline_text(self, *args, **kwargs): 
                    return None
                def index_pdf_bytes(self, *args, **kwargs): 
                    return type('obj', (object,), {
                        'id': str(__import__('uuid').uuid4()),
                        'name': args[1] if len(args) > 1 else 'unknown',
                        'type': 'PDF',
                        'pages': None,
                        'preview': '',
                        'chunks': [],
                        'content_hash': None,
                        'status': 'disabled',
                        'message': 'RAG disabled'
                    })()
                def index_text_bytes(self, *args, **kwargs): 
                    return self.index_pdf_bytes(*args, **kwargs)
                def register_stub(self, name, type_, message): 
                    return type('obj', (object,), {
                        'id': str(__import__('uuid').uuid4()),
                        'name': name,
                        'type': type_,
                        'pages': None,
                        'preview': message,
                        'chunks': [],
                        'content_hash': None,
                        'status': 'stub',
                        'message': message
                    })()
                def search(self, *args, **kwargs): 
                    return []
            _rag_store = DummyRagStore()
    return _rag_store



class Message(BaseModel):
    role: str
    content: str


class Document(BaseModel):
    id: Optional[str]
    name: Optional[str]
    type: Optional[str]
    pages: Optional[Union[int, str]] = None
    tags: Optional[List[str]] = Field(default_factory=list)
    content: Optional[str] = ""
    image: Optional[str] = None
    image_mime: Optional[str] = None
    tag_key: Optional[str] = None


class SystemContext(BaseModel):
    """ç³»çµ±ç•¶å‰ç‹€æ…‹è³‡è¨Š"""
    case_id: Optional[str] = None
    owner_name: Optional[str] = None
    has_summary: bool = False
    has_translation: bool = False
    has_memo: bool = False
    translation_count: int = 0
    selected_doc_id: Optional[str] = None
    selected_doc_name: Optional[str] = None


class RouteDecision(BaseModel):
    mode: Literal["simple", "full"] = "full"
    needs_web_search: bool = False
    needs_rag: bool = False
    needs_vision: bool = False
    reason: Optional[str] = None


class ArtifactRequest(BaseModel):
    messages: List[Message] = Field(default_factory=list)
    documents: List[Document] = Field(default_factory=list)
    stream: bool = False
    system_context: Optional[SystemContext] = None


class TagUpdateRequest(BaseModel):
    tag_key: Optional[str] = None
    tags: Optional[List[str]] = None
    custom_tags: Optional[List[str]] = None


# ç™»å½•ç›¸å…³æ•°æ®æ¨¡å‹
class LoginRequest(BaseModel):
    username: str
    password: str


class LoginResponse(BaseModel):
    success: bool
    token: Optional[str] = None
    error: Optional[str] = None


# Simple in-memory session store (for production, use Redis or database)
active_sessions: Dict[str, datetime] = {}
SESSION_TIMEOUT = timedelta(hours=24)


def create_session_token() -> str:
    """ç”Ÿæˆå®‰å…¨çš„ä¼šè¯ä»¤ç‰Œ"""
    return secrets.token_urlsafe(32)


def verify_session(token: str) -> bool:
    """éªŒè¯ä¼šè¯ä»¤ç‰Œæ˜¯å¦æœ‰æ•ˆ"""
    if token not in active_sessions:
        return False
    
    if datetime.now() > active_sessions[token]:
        # Tokenè¿‡æœŸï¼Œåˆ é™¤
        del active_sessions[token]
        return False
    
    return True


def cleanup_expired_sessions():
    """æ¸…ç†è¿‡æœŸçš„ä¼šè¯"""
    now = datetime.now()
    expired = [token for token, expiry in active_sessions.items() if now > expiry]
    for token in expired:
        del active_sessions[token]


def get_model_id() -> str:
    return os.getenv("OPENAI_MODEL", "gpt-4o-mini")


WEB_SEARCH_TOOL = {"type": "web_search_preview"}
IMAGE_EXTENSIONS = {".png", ".jpg", ".jpeg", ".webp", ".gif"}
TRACE_MAX_LEN = int(os.getenv("AGNO_TRACE_MAX_LEN", "2000"))
TRACE_ARGS_MAX_LEN = int(os.getenv("AGNO_TRACE_ARGS_MAX_LEN", "1000"))
STORE_EVENTS = os.getenv("AGNO_STORE_EVENTS", "").lower() in {"1", "true", "yes", "on"}
DEFAULT_REASONING_EFFORT = os.getenv("OPENAI_REASONING_EFFORT", "medium")
# å•Ÿç”¨æ¨ç†æ‘˜è¦ä»¥é¡¯ç¤º LLM æ€è€ƒéç¨‹ï¼ˆGPT-5.2 æ”¯æŒï¼‰
DEFAULT_REASONING_SUMMARY = os.getenv("OPENAI_REASONING_SUMMARY", "auto").strip()
USE_RESPONSES_MODEL = os.getenv("OPENAI_USE_RESPONSES", "1").lower() in {"1", "true", "yes", "on"}
# ç´¢å¼•æ–°è/ç ”ç©¶çµæœåˆ° RAG æœƒè§¸ç™¼å¤§é‡ embedding API å‘¼å«ï¼Œå°æœå°‹é€Ÿåº¦å½±éŸ¿æ¥µå¤§
# é è¨­é—œé–‰ï¼Œå¦‚éœ€å¾ŒçºŒ RAG æª¢ç´¢è«‹åœ¨ç’°å¢ƒè®Šæ•¸ä¸­å•Ÿç”¨
INDEX_WEB_SEARCH_DOCS = os.getenv("AGNO_INDEX_WEB_SEARCH_DOCS", "0").lower() in {"1", "true", "yes", "on"}
_RAG_INDEX_LOCK = threading.Lock()


def get_model(
    enable_web_search: bool = False,
    enable_vision: bool = False,
    model_id: Optional[str] = None,
) -> Any:
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise RuntimeError("OPENAI_API_KEY æœªè¨­å®šï¼Œç„¡æ³•å‘¼å«æ¨¡å‹")

    model_name = model_id or get_model_id()

    reasoning_opts: Dict[str, Any] = {}
    if DEFAULT_REASONING_SUMMARY:
        reasoning_opts["summary"] = DEFAULT_REASONING_SUMMARY
    if DEFAULT_REASONING_EFFORT:
        reasoning_opts["effort"] = DEFAULT_REASONING_EFFORT

    # Prefer Responses API to surface reasoning summary (needed for routing display)
    use_responses = enable_web_search or USE_RESPONSES_MODEL
    if use_responses:
        return OpenAIResponses(
            id=model_name,
            api_key=api_key,
            reasoning=reasoning_opts or None,
            reasoning_effort=DEFAULT_REASONING_EFFORT or None,
            reasoning_summary=DEFAULT_REASONING_SUMMARY or None,
        )

    kwargs: Dict[str, Any] = {
        "id": model_name,
        "api_key": api_key,
        "reasoning_effort": DEFAULT_REASONING_EFFORT,
    }
    # Vision inputs are passed via Agent.run(images=...), no extra request params needed.

    return OpenAIChat(**kwargs)


def get_research_model_id() -> str:
    return os.getenv("OPENAI_RESEARCH_MODEL", get_model_id())


def get_router_model_id() -> str:
    return os.getenv("OPENAI_ROUTER_MODEL", get_model_id())


def build_system_status(
    documents: List[Document], system_context: Optional[SystemContext]
) -> str:
    """æ§‹å»ºç³»çµ±ç•¶å‰ç‹€æ…‹æ‘˜è¦ï¼Œè®“ LLM äº†è§£ç³»çµ±ä¸­å·²æœ‰å“ªäº›è³‡æ–™"""
    lines = []

    # æ¡ˆä»¶è³‡è¨Š
    if system_context:
        if system_context.case_id:
            lines.append(f"ã€æ¡ˆä»¶ç·¨è™Ÿã€‘{system_context.case_id}")
        if system_context.owner_name:
            lines.append(f"ã€è² è²¬äººã€‘{system_context.owner_name}")
        if system_context.selected_doc_name:
            lines.append(f"ã€ç›®å‰é¸å–æ–‡ä»¶ã€‘{system_context.selected_doc_name}")
        elif system_context.selected_doc_id:
            lines.append(f"ã€ç›®å‰é¸å–æ–‡ä»¶ã€‘{system_context.selected_doc_id}")

    # æ–‡ä»¶æ¸…å–®
    if documents:
        doc_list = []
        for idx, doc in enumerate(documents, start=1):
            pages = doc.pages if doc.pages not in (None, "") else "-"
            tags = f" (æ¨™ç±¤: {', '.join(doc.tags)})" if doc.tags else ""
            image_hint = " (å½±åƒ)" if doc.image else ""
            doc_list.append(
                f"  {idx}. {doc.name or 'æœªå‘½å'} [{doc.type or 'FILE'}] - {pages}é {tags}{image_hint}"
            )
        lines.append(f"ã€å·²ä¸Šå‚³æ–‡ä»¶ã€‘å…± {len(documents)} ä»½:")
        lines.extend(doc_list)
    else:
        lines.append("ã€å·²ä¸Šå‚³æ–‡ä»¶ã€‘ç„¡")

    # Artifacts ç‹€æ…‹
    if system_context:
        artifact_status = []
        if system_context.has_summary:
            artifact_status.append("æ‘˜è¦")
        if system_context.translation_count > 0:
            artifact_status.append(f"ç¿»è­¯ ({system_context.translation_count} ä»½)")
        if system_context.has_memo:
            artifact_status.append("æˆä¿¡å ±å‘Š")

        if artifact_status:
            lines.append(f"ã€å·²ç”¢ç”Ÿ Artifactsã€‘{', '.join(artifact_status)}")
        else:
            lines.append("ã€å·²ç”¢ç”Ÿ Artifactsã€‘ç„¡")

    return "\n".join(lines)


def build_doc_context(
    documents: List[Document],
    selected_doc_id: Optional[str] = None,
    include_content: bool = True,
) -> str:
    if not documents:
        return "æ–‡ä»¶æ¸…å–®: ç„¡ã€‚"

    lines = []
    for idx, doc in enumerate(documents, start=1):
        tags = "ã€".join(doc.tags or []) if doc.tags else "ç„¡"
        pages = doc.pages if doc.pages not in (None, "") else "-"
        if include_content:
            content = (doc.content or "").strip()
            stored = get_rag_store().docs.get(doc.id or "") if doc.id else None
            if not content and stored and stored.preview:
                content = f"PDF å·²ç´¢å¼•ï¼ˆå¯ RAG æª¢ç´¢ï¼‰ã€‚é è¦½ï¼š{stored.preview}"
            if doc.image:
                safe_content = "å½±åƒæª”ï¼Œç„¡æ–‡å­—æ‘˜è¦ã€‚"
            else:
                safe_content = content[:2000] if content else "æœªæä¾›"
        else:
            safe_content = "å…§å®¹å·²çœç•¥ï¼ˆæœå°‹æ¨¡å¼ï¼‰"
        image_hint = "   å½±åƒ: å·²æä¾›ï¼ˆå¯ç”¨ Vision Agent è§£æï¼‰" if doc.image else None
        selected_mark = " (ç›®å‰é¸å–)" if selected_doc_id and doc.id == selected_doc_id else ""
        lines.append(
            "\n".join(
                [
                    f"{idx}. åç¨±: {doc.name or 'æœªå‘½å'}{selected_mark}",
                    f"   id: {doc.id or '-'}",
                    f"   é¡å‹: {doc.type or '-'}",
                    f"   é æ•¸: {pages}",
                    f"   æ¨™ç±¤: {tags}",
                    f"   å…§å®¹æ‘˜è¦: {safe_content}",
                    *([image_hint] if image_hint else []),
                ]
            )
        )
    return "æ–‡ä»¶æ¸…å–®:\n" + "\n".join(lines)


def build_image_inputs(documents: List[Document]) -> List[Image]:
    images: List[Image] = []
    for doc in documents:
        raw = (doc.image or "").strip()
        if not raw:
            continue
        mime = doc.image_mime
        payload = raw
        if raw.startswith("data:"):
            header, _, data_part = raw.partition(",")
            payload = data_part or ""
            if not mime:
                mime = header.split(";")[0].replace("data:", "").strip() or None
        if not payload:
            continue
        try:
            content = base64.b64decode(payload)
        except Exception:
            continue
        images.append(
            Image(
                content=content,
                mime_type=mime,
                id=doc.id,
                alt_text=doc.name,
            )
        )
    return images


def index_rag_async(doc_id: str, name: str, content: str, doc_type: str) -> None:
    if not INDEX_WEB_SEARCH_DOCS:
        return

    store = get_rag_store()

    def _task():
        try:
            with _RAG_INDEX_LOCK:
                store.index_inline_text(doc_id, name, content, doc_type)
        except Exception as exc:
            print(f"[WARN] RAG ç´¢å¼•å¤±æ•—: {doc_type} {name}: {exc}")

    threading.Thread(target=_task, daemon=True).start()


def run_ocr_for_documents(documents: List[Document]) -> List[Dict[str, Any]]:
    updates: List[Dict[str, Any]] = []
    if not documents:
        return updates

    agent = build_vision_agent()
    for doc in documents:
        content = (doc.content or "").strip()
        if content or not doc.image:
            continue
        if not doc.id:
            doc.id = str(uuid.uuid4())
        images = build_image_inputs([doc])
        if not images:
            continue
        try:
            prompt = "è«‹é‡å°é€™å¼µåœ–ç‰‡åš OCRï¼Œè¼¸å‡ºç´”æ–‡å­—å…§å®¹ï¼Œä¸è¦åŠ å…¥å¤šé¤˜èªªæ˜ã€‚"
            resp = agent.run(prompt, images=images)
            text = (resp.get_content_as_string() or "").strip()
            if not text:
                continue
            doc.content = text
            get_rag_store().index_inline_text(doc.id, doc.name or doc.id, text, doc.type or "IMAGE")
            updates.append(
                {
                    "id": doc.id,
                    "name": doc.name or "æœªå‘½å",
                    "type": doc.type or "IMAGE",
                    "pages": estimate_pages(text),
                    "content": text,
                    "preview": text[:400],
                    "status": "indexed",
                    "message": "",
                    "tag_key": doc.tag_key,
                    "tags": doc.tags or [],
                }
            )
        except Exception as exc:
            updates.append(
                {
                    "id": doc.id,
                    "name": doc.name or "æœªå‘½å",
                    "type": doc.type or "IMAGE",
                    "pages": doc.pages or "-",
                    "content": doc.content or "",
                    "preview": doc.content[:400] if doc.content else "",
                    "status": "error",
                    "message": str(exc),
                    "tag_key": doc.tag_key,
                    "tags": doc.tags or [],
                }
            )
    return updates


def build_conversation(messages: List[Message]) -> str:
    if not messages:
        return "å°è©±ç´€éŒ„ï¼šç„¡ã€‚"
    parts = []
    for msg in messages:
        content = (msg.content or "").strip()
        if not content:
            continue
        parts.append(f"{msg.role}: {content}")
    return "å°è©±ç´€éŒ„:\n" + "\n".join(parts) if parts else "å°è©±ç´€éŒ„ï¼šç„¡ã€‚"


def get_last_user_message(messages: List[Message]) -> str:
    for msg in reversed(messages or []):
        content = (msg.content or "").strip()
        if msg.role == "user" and content:
            return content
    return ""


def build_empty_response(message: str) -> Dict[str, Any]:
    return {
        "assistant": {"content": message, "bullets": []},
        "summary": {
            "output": "",
            "borrower": {"name": "", "description": "", "rating": ""},
            "metrics": [],
            "risks": [],
            "source_doc_id": "",
            "source_doc_ids": [],
        },
        "translation": {"output": "", "clauses": [], "source_doc_id": "", "source_doc_ids": []},
        "memo": {
            "output": "",
            "sections": [],
            "recommendation": "",
            "conditions": "",
        },
        "routing": [],
    }


def compute_tag_key(data: bytes) -> str:
    return hashlib.md5(data).hexdigest()


def estimate_pages(content: str) -> int:
    if not content:
        return 1
    return max(1, (len(content) + 2999) // 3000)


def parse_news_section(section: str) -> Optional[Dict[str, str]]:
    import re

    if not section.strip():
        return None

    lines = section.strip().split('\n')
    if len(lines) < 2:
        return None

    title = lines[0].strip()
    article_content = '\n'.join(lines[1:]).strip()

    # éæ¿¾æ‰ç³»çµ±ä¿¡æ¯ï¼šæª¢æŸ¥æ¨™é¡Œæ˜¯å¦åŒ…å«ç³»çµ±ç›¸é—œé—œéµè©
    system_keywords = ['æ¡ˆä»¶', 'CASE', 'æœƒè©±', 'æª¢ç´¢', 'ID', 'ç·¨è™Ÿ', 'ç³»çµ±', 'åŠ©ç†', 'æˆ‘æ˜¯', 'æˆ‘å¯ä»¥']
    if any(keyword in title for keyword in system_keywords):
        return None

    # æå–ç™¼å¸ƒæ™‚é–“
    publish_date = ""
    date_match = re.search(r'ç™¼å¸ƒæ™‚é–“[ï¼š:]\s*(\d{4}[-å¹´]\d{1,2}[-æœˆ]\d{1,2}æ—¥?)', article_content)
    if date_match:
        publish_date = date_match.group(1)

    # æå– URL
    url = ""
    url_match = re.search(r'https?://[^\s\)]+', article_content)
    if url_match:
        url = url_match.group(0)

    # é©—è­‰æ˜¯å¦ç‚ºæœ‰æ•ˆæ–°èï¼šå¿…é ˆæœ‰ URL æˆ–ç™¼å¸ƒæ™‚é–“
    if not url and not publish_date:
        return None

    # é©—è­‰æ¨™é¡Œé•·åº¦ï¼ˆå¤ªçŸ­æˆ–å¤ªé•·éƒ½å¯èƒ½ä¸æ˜¯æ–°èæ¨™é¡Œï¼‰
    if len(title) < 5 or len(title) > 200:
        return None

    # é©—è­‰å…§å®¹é•·åº¦ï¼ˆå¤ªçŸ­å¯èƒ½ä¸æ˜¯å®Œæ•´æ–°èï¼‰
    if len(article_content) < 30:
        return None

    return {
        'title': title,
        'content': article_content,
        'publish_date': publish_date,
        'url': url
    }


def parse_news_articles(content: str) -> List[Dict[str, str]]:
    """è§£ææ–°èå…§å®¹ï¼Œè¿”å›ç¨ç«‹æ–°èåˆ—è¡¨"""
    import re

    articles: List[Dict[str, str]] = []
    # ä½¿ç”¨ ### ä½œç‚ºæ–°èåˆ†éš”ç¬¦
    sections = re.split(r'\n###\s+', content)

    for section in sections:
        article = parse_news_section(section)
        if article:
            articles.append(article)

    return articles


def parse_news_articles_streaming(content: str) -> List[Dict[str, str]]:
    """æµå¼è§£æï¼šåªå›å‚³å·²å®Œæˆçš„æ–°èï¼ˆæ’é™¤æœ€å¾Œä¸€æ®µæœªçµæŸçš„ sectionï¼‰"""
    import re

    sections = re.split(r'\n###\s+', content)
    if len(sections) <= 2:
        return []

    complete_sections = sections[:-1]
    articles: List[Dict[str, str]] = []
    for section in complete_sections:
        article = parse_news_section(section)
        if article:
            articles.append(article)
    return articles


def extract_assistant_content_from_json(raw: str) -> str:
    """å¾å°šæœªå®Œæˆçš„ JSON å­—ä¸²ä¸­è§£æ assistant.contentï¼ˆå®¹éŒ¯ã€ä¸é˜»å¡ï¼‰"""
    if not raw:
        return ""
    idx = raw.find('"assistant"')
    if idx == -1:
        return ""
    idx = raw.find('"content"', idx)
    if idx == -1:
        return ""
    idx = raw.find(":", idx)
    if idx == -1:
        return ""
    i = idx + 1
    length = len(raw)
    while i < length and raw[i] in " \t\r\n":
        i += 1
    if i >= length or raw[i] != '"':
        return ""
    i += 1
    out: List[str] = []
    while i < length:
        ch = raw[i]
        if ch == "\\":
            if i + 1 >= length:
                break
            nxt = raw[i + 1]
            if nxt in {'"', "\\", "/"}:
                out.append(nxt)
                i += 2
                continue
            if nxt == "n":
                out.append("\n")
                i += 2
                continue
            if nxt == "r":
                out.append("\r")
                i += 2
                continue
            if nxt == "t":
                out.append("\t")
                i += 2
                continue
            if nxt == "b":
                out.append("\b")
                i += 2
                continue
            if nxt == "f":
                out.append("\f")
                i += 2
                continue
            if nxt == "u" and i + 5 < length:
                hex_str = raw[i + 2:i + 6]
                try:
                    out.append(chr(int(hex_str, 16)))
                    i += 6
                    continue
                except Exception:
                    pass
            out.append(nxt)
            i += 2
            continue
        if ch == '"':
            break
        out.append(ch)
        i += 1
    return "".join(out)


def make_news_key(article: Dict[str, str]) -> str:
    title = (article.get("title") or "").strip()
    publish_date = (article.get("publish_date") or "").strip()
    url = (article.get("url") or "").strip()
    if not title:
        return ""
    return f"{title.lower()}|{publish_date}|{url.lower()}"


def make_news_doc_id(news_key: str) -> str:
    digest = hashlib.md5(news_key.encode("utf-8")).hexdigest()
    return f"news-{digest}"


def build_news_records_from_articles(
    articles: List[Dict[str, str]],
    seen_keys: Optional[set] = None,
) -> List[Dict[str, Any]]:
    if not articles:
        return []
    seen = seen_keys if seen_keys is not None else set()
    new_articles: List[Dict[str, str]] = []
    new_keys: List[str] = []
    for article in articles:
        key = make_news_key(article)
        if not key:
            continue
        if key in seen:
            continue
        seen.add(key)
        new_keys.append(key)
        new_articles.append(article)

    if not new_articles:
        return []

    titles = [article.get("title") for article in new_articles if article.get("title")]
    unique_titles = list(dict.fromkeys(titles))
    translations = batch_translate_titles(unique_titles) if unique_titles else {}

    documents = []
    for key, article in zip(new_keys, new_articles):
        doc_id = make_news_doc_id(key)
        original_title = article["title"]
        content = article["content"]
        publish_date = article["publish_date"]
        url = article["url"]

        title = translations.get(original_title, original_title)

        # çµ„åˆå®Œæ•´å…§å®¹ç”¨æ–¼åœ‹å®¶åˆ¤æ–·
        full_content = f"# {title}\n\n"
        if publish_date:
            full_content += f"**ç™¼å¸ƒæ™‚é–“**: {publish_date}\n\n"
        full_content += content
        if url:
            full_content += f"\n\n**ä¾†æº**: {url}"

        # åˆ¤æ–·ä¾†æºåœ‹å®¶
        country = extract_country_from_content(full_content, fallback_name=title)

        # ç´¢å¼•åˆ° RAGï¼ˆèƒŒæ™¯åŸ·è¡Œï¼Œé¿å…é˜»å¡ï¼‰
        index_rag_async(doc_id, title, full_content, "NEWS")

        # å‰µå»ºæ–‡ä»¶è¨˜éŒ„ï¼ˆä½¿ç”¨ç¿»è­¯å¾Œçš„æ¨™é¡Œï¼‰
        document_record = {
            "id": doc_id,
            "name": title,  # å·²ç¿»è­¯çš„æ¨™é¡Œ
            "type": "NEWS",
            "pages": estimate_pages(full_content),
            "status": "indexed",
            "message": "",
            "preview": content[:300],
            "content": full_content,
            "source": "news",
            "tags": [country] if country and country != " " else [],  # å°‡åœ‹å®¶ä½œç‚ºæ¨™ç±¤
            "country": country,  # ä¿å­˜åœ‹å®¶å­—æ®µ
            "publish_date": publish_date,
            "url": url,
        }

        # ä¿å­˜åˆ°æ•¸æ“šåº«
        news_store.add_record(document_record)
        documents.append(document_record)

    return documents


def build_research_document(
    data: Dict[str, Any],
    last_user: str,
    use_web_search: bool,
) -> Optional[Dict[str, Any]]:
    if not use_web_search:
        return None

    content_parts: List[str] = []
    assistant_content = (data.get("assistant") or {}).get("content") or ""
    summary_output = (data.get("summary") or {}).get("output") or ""
    memo_output = (data.get("memo") or {}).get("output") or ""
    translation_output = (data.get("translation") or {}).get("output") or ""

    if assistant_content:
        content_parts.append(f"## å›è¦†é‡é»\n{assistant_content}")
    if summary_output:
        # ç§»é™¤æ‘˜è¦ä¸­çš„åœ‹å®¶åç¨±æ¨™é¡Œï¼ˆå¦‚ ##è²å¾‹è³“ã€##æ³°åœ‹ Thailand ç­‰ï¼‰
        cleaned_summary = re.sub(r'##\s*(è¶Šå—|æ³°åœ‹|å°å°¼|è²å¾‹è³“|æŸ¬åŸ”å¯¨|æ–°åŠ å¡|é¦¬ä¾†è¥¿äº|ç·¬ç”¸|å¯®åœ‹|æ±å—äº)(\s+[A-Za-z]+)?\s*\n*', '', summary_output)
        content_parts.append(f"## æ‘˜è¦\n{cleaned_summary}")
    if memo_output:
        content_parts.append(f"## Credit Memo\n{memo_output}")
    if translation_output:
        content_parts.append(f"## ç¿»è­¯\n{translation_output}")

    if not content_parts:
        return None

    combined = "\n\n".join(content_parts).strip()
    if not combined:
        return None

    title_hint = (last_user or "Research").strip().replace("\n", " ")
    title_hint = title_hint[:28] + "..." if len(title_hint) > 28 else title_hint
    name = f"Deep Research - {title_hint or 'Research'}"
    doc_id = str(uuid.uuid4())

    index_rag_async(doc_id, name, combined, "RESEARCH")

    # å‰µå»ºæ–‡ä»¶è¨˜éŒ„
    document_record = {
        "id": doc_id,
        "name": name,
        "type": "RESEARCH",
        "pages": estimate_pages(combined),
        "status": "indexed",
        "message": "",
        "preview": combined[:400],
        "content": combined,
        "source": "research",
        "tags": []
    }
    
    # ä¿å­˜åˆ°æ•¸æ“šåº«
    news_store.add_record(document_record)
    
    return document_record


def build_news_documents(
    data: Dict[str, Any],
    last_user: str,
    use_web_search: bool,
    seen_keys: Optional[set] = None,
) -> List[Dict[str, Any]]:
    """å°‡æœå°‹çµæœæ‹†åˆ†æˆç¨ç«‹çš„æ–°èæ–‡æª”"""
    if not use_web_search:
        return []
    
    assistant_content = (data.get("assistant") or {}).get("content") or ""
    if not assistant_content:
        return []
    
    # è§£ææ–°èåˆ—è¡¨
    articles = parse_news_articles(assistant_content)
    if not articles:
        return []
    
    return build_news_records_from_articles(articles, seen_keys=seen_keys)


def build_smalltalk_agent(
    documents: List[Document],
    system_context: Optional[SystemContext],
) -> Agent:
    system_status = build_system_status(documents, system_context)
    return Agent(
        name="ChitChat",
        role="ç°¡çŸ­ä¸”è¦ªåˆ‡çš„æ–°èæƒ…å ±åŠ©ç†ï¼Œåƒ…åšå¯’æš„æˆ–ç¢ºèªéœ€æ±‚ï¼Œä¸è¦ä¸»å‹•ç”Ÿæˆå ±å‘Šã€‚",
        model=get_model(),
        store_events=STORE_EVENTS,
        instructions=[
            "ä½ æ˜¯æ±å—äºæ–°èæƒ…å ±åŠ©ç†ï¼Œå¯ä»¥å”åŠ©æ–°èæª¢ç´¢ã€æƒ…å ±åˆ†æã€æ–‡ä»¶æ‘˜è¦ç­‰å·¥ä½œã€‚",
            "è«‹åƒè€ƒå°è©±ç´€éŒ„å»¶çºŒè„ˆçµ¡ï¼Œé¿å…å¿½ç•¥å…ˆå‰å…§å®¹ã€‚",
            "ç•¶ç”¨æˆ¶è©¢å•ã€Œç›®å‰æœ‰å“ªäº›æ–°èã€æˆ–ã€Œç³»çµ±ç‹€æ…‹ã€æ™‚ï¼Œè«‹æ ¹æ“šä¸‹æ–¹ç³»çµ±ç‹€æ…‹è³‡è¨Šå›ç­”ã€‚",
            "ä¿æŒä¸€å¥æˆ–å…©å¥çš„è‡ªç„¶å›æ‡‰ï¼Œç¢ºèªéœ€æ±‚å³å¯ã€‚",
            "ä¸è¦æ‰¿è«¾é–‹å§‹ç”¢å‡ºå ±å‘Šæˆ–åˆ†æï¼›è«‹è©¢å•ä½¿ç”¨è€…éœ€è¦ä»€éº¼å”åŠ©ã€‚",
            "èªæ°£å‹å–„ã€ç°¡æ½”ï¼Œé¿å…å†—é•·ã€‚",
            "",
            f"ã€ç³»çµ±ç•¶å‰ç‹€æ…‹ã€‘\n{system_status}",
        ],
        markdown=False,
    )


def build_router_agent(
    documents: List[Document],
    system_context: Optional[SystemContext],
) -> Agent:
    system_status = build_system_status(documents, system_context)
    return Agent(
        name="Router",
        role="åˆ¤æ–·ä½¿ç”¨è€…éœ€æ±‚è¦èµ°å“ªç¨®è™•ç†æ¨¡å¼",
        model=get_model(model_id=get_router_model_id()),
        instructions=[
            "ä½ æ˜¯è·¯ç”±å™¨ï¼Œè² è²¬åˆ¤æ–·æ˜¯å¦éœ€è¦ç°¡å–®å›è¦†æˆ–å®Œæ•´è™•ç†ã€‚",
            "è«‹è¼¸å‡º JSONï¼Œç¬¦åˆ schemaï¼š",
            '{ "mode": "simple|full", "needs_web_search": true|false, "needs_rag": true|false, "needs_vision": true|false, "reason": "ç°¡çŸ­åŸå› " }',
            "åƒ…åœ¨å•å€™/å¯’æš„/è‡´è¬ä¸”ä¸éœ€è¦å·¥å…·æ™‚æ‰å› simpleã€‚",
            "è‹¥éœ€è¦æœ€æ–°/å¤–éƒ¨è³‡è¨Š â†’ needs_web_search = trueã€‚",
            "è‹¥éœ€è¦è®€å–æˆ–æ‘˜è¦/ç¿»è­¯ä½¿ç”¨è€…ä¸Šå‚³æ–‡ä»¶ â†’ needs_rag = trueã€‚",
            "è‹¥éœ€è¦è§£æå½±åƒ/æˆªåœ–/æƒæä»¶ â†’ needs_vision = trueã€‚",
            "ä¸å…è¨±è¼¸å‡ºå¤šé¤˜æ–‡å­—ï¼Œåªèƒ½è¼¸å‡º JSONã€‚",
            "",
            f"ã€ç³»çµ±ç•¶å‰ç‹€æ…‹ã€‘\n{system_status}",
        ],
        markdown=False,
    )


def build_smalltalk_prompt(messages: List[Message]) -> str:
    convo = build_conversation(messages)
    last_user = get_last_user_message(messages)
    if last_user:
        return f"{convo}\n\nä½¿ç”¨è€…æœ€æ–°è¨Šæ¯ï¼š{last_user}\n\nè«‹æ ¹æ“šå°è©±ç´€éŒ„ç°¡çŸ­å›è¦†ã€‚"
    return f"{convo}\n\nè«‹ç°¡çŸ­å›è¦†ã€‚"


def run_smalltalk_agent(
    messages: List[Message],
    documents: List[Document],
    system_context: Optional[SystemContext],
) -> str:
    """Use a lightweight chat agent to handle greetings/smalltalk via Agno."""
    agent = build_smalltalk_agent(documents, system_context)
    try:
        prompt = build_smalltalk_prompt(messages)
        resp = agent.run(prompt)
        return resp.get_content_as_string()
    except Exception:
        # fallback to static short response
        return "ä½ å¥½ï¼æˆ‘æ˜¯æ±å—äºæ–°èæƒ…å ±åŠ©ç†ï¼Œå¯ä»¥å”åŠ©æ–°èæª¢ç´¢ã€æƒ…å ±åˆ†æèˆ‡æ‘˜è¦ã€‚è«‹å‘Šè¨´æˆ‘éœ€è¦ä»€éº¼å”åŠ©ï¼Ÿ"


def quick_route_check(messages: List[Message]) -> Optional[str]:
    """å¿«é€Ÿé—œéµè©æª¢æŸ¥ï¼Œé¿å…ä¸å¿…è¦çš„ LLM è·¯ç”±åˆ¤æ–·"""
    if not messages:
        return None
    
    last_msg = get_last_user_message(messages)
    if not last_msg:
        return None
    
    msg_lower = last_msg.lower()
    
    # æ˜ç¢ºçš„ä»»å‹™é—œéµè© â†’ ç›´æ¥èµ° full æ¨¡å¼
    task_keywords = ['æ–°è', 'æœå°‹', 'æŸ¥è©¢', 'æ‰¾', 'åˆ†æ', 'æ‘˜è¦', 'ç¿»è­¯', 'å ±å‘Š', 'æœ€è¿‘', 'åœ‹å®¶', 'ç”¢æ¥­', 'ç¶“æ¿Ÿ']
    if any(keyword in msg_lower for keyword in task_keywords):
        print(f"âš¡ [å¿«é€Ÿè·¯ç”±] æª¢æ¸¬åˆ°ä»»å‹™é—œéµè©ï¼Œç›´æ¥ä½¿ç”¨ full æ¨¡å¼")
        return "full"
    
    # ç°¡å–®å•å€™ â†’ simple æ¨¡å¼
    greetings = ['ä½ å¥½', 'hi', 'hello', 'å—¨', 'æ—©å®‰', 'åˆå®‰', 'æ™šå®‰', 'è¬è¬', 'thanks', 'æ„Ÿè¬']
    if any(greeting in msg_lower for greeting in greetings) and len(msg_lower) < 20:
        print(f"âš¡ [å¿«é€Ÿè·¯ç”±] æª¢æ¸¬åˆ°å•å€™èªï¼Œä½¿ç”¨ simple æ¨¡å¼")
        return "simple"
    
    return None


def run_router_agent(
    messages: List[Message],
    documents: List[Document],
    system_context: Optional[SystemContext],
) -> Optional[RouteDecision]:
    if not messages:
        return None
    
    # å¿«é€Ÿè·¯ç”±æª¢æŸ¥
    quick_mode = quick_route_check(messages)
    if quick_mode == "full":
        # ç›´æ¥è¿”å› full æ¨¡å¼ï¼Œè·³é LLM èª¿ç”¨
        return RouteDecision(
            mode="full",
            needs_web_search=True,
            needs_rag=False,
            needs_vision=False,
            reason="ä»»å‹™é—œéµè©æª¢æ¸¬"
        )
    elif quick_mode == "simple":
        return RouteDecision(
            mode="simple",
            needs_web_search=False,
            needs_rag=False,
            needs_vision=False,
            reason="å•å€™èªæª¢æ¸¬"
        )
    
    # ç„¡æ³•å¿«é€Ÿåˆ¤æ–·ï¼Œä½¿ç”¨ LLM è·¯ç”±
    try:
        print(f"ğŸ¤” [LLMè·¯ç”±] ä½¿ç”¨æ¨¡å‹åˆ¤æ–·è·¯ç”±")
        router = build_router_agent(documents, system_context)
        convo = build_conversation(messages)
        prompt = f"{convo}\n\nè«‹åˆ¤æ–·è·¯ç”±ä¸¦è¼¸å‡º JSONã€‚"
        resp = router.run(prompt, output_schema=RouteDecision)
        content = getattr(resp, "content", None)
        if isinstance(content, RouteDecision):
            return content
        if isinstance(content, dict):
            return RouteDecision(**content)
        text = resp.get_content_as_string()
        if text:
            return RouteDecision.model_validate_json(text)
    except Exception as e:
        print(f"âŒ [è·¯ç”±éŒ¯èª¤] {e}")
        return None
    return None


def extract_stream_text(event: Any) -> Optional[str]:
    if isinstance(event, str):
        return event
    if hasattr(event, "get_content_as_string") and not hasattr(event, "event"):
        content = event.get_content_as_string()
        return content if content else None
    event_name = getattr(event, "event", "") or ""
    if event_name in {
        "TeamRunContent",
        "TeamRunIntermediateContent",
        "RunContent",
        "RunIntermediateContent",
    }:
        content = getattr(event, "content", None)
        if content is None:
            return None
        if isinstance(content, str):
            return content
        try:
            return json.dumps(content, ensure_ascii=False)
        except TypeError:
            return str(content)
    return None


def extract_reasoning_text(event: Any) -> Optional[str]:
    """æå–æ¨ç†éç¨‹æ–‡æœ¬ï¼ŒåŒ…æ‹¬ GPT-5.2 çš„æ¨ç†æ‘˜è¦"""
    if event is None:
        return None
    
    # å˜—è©¦å¾äº‹ä»¶ä¸­æå–æ¨ç†æ‘˜è¦
    summary_text = getattr(event, "reasoning_summary", None)
    if summary_text:
        return truncate_text(summary_text, TRACE_MAX_LEN)
    
    event_name = getattr(event, "event", "") or ""
    if event_name in {
        TeamRunEvent.reasoning_started.value,
        RunEvent.reasoning_started.value,
        TeamRunEvent.reasoning_step.value,
        RunEvent.reasoning_step.value,
        TeamRunEvent.reasoning_content_delta.value,
        RunEvent.reasoning_content_delta.value,
        TeamRunEvent.reasoning_completed.value,
        RunEvent.reasoning_completed.value,
    }:
        reasoning_content = getattr(event, "reasoning_content", None) or getattr(event, "content", None)
        steps_text = format_reasoning_steps(getattr(event, "reasoning_steps", None))
        text = reasoning_content or steps_text
        if text:
            return truncate_text(text, TRACE_MAX_LEN)
    
    # æª¢æŸ¥æ˜¯å¦æœ‰ reasoning ç›¸é—œçš„è¼¸å‡ºé …ç›®ï¼ˆResponses APIï¼‰
    if hasattr(event, "output") and isinstance(event.output, list):
        for item in event.output:
            if isinstance(item, dict) and item.get("type") == "reasoning":
                summary_items = item.get("summary", [])
                for summary_item in summary_items:
                    if isinstance(summary_item, dict) and summary_item.get("type") == "summary_text":
                        text = summary_item.get("text", "")
                        if text:
                            return truncate_text(text, TRACE_MAX_LEN)
    
    return None


def build_reasoning_summary(chunks: List[str]) -> str:
    for text in reversed(chunks):
        clean = (text or "").strip()
        if clean:
            return truncate_text(clean, TRACE_MAX_LEN)
    return ""


def format_tool_label(tool_name: Optional[str]) -> str:
    if not tool_name:
        return "å·¥å…·å‘¼å«"
    normalized = tool_name.strip()
    lower = normalized.lower()
    if "web_search" in lower:
        return "ç¶²è·¯æŸ¥è©¢"
    if "search_knowledge" in lower or "knowledge" in lower:
        return "æ–‡ä»¶æª¢ç´¢"
    return normalized.replace("_", " ")


def build_routing_update(event: Any, routing_state: Dict[str, str]) -> Optional[Dict[str, str]]:
    event_name = getattr(event, "event", "") or ""
    
    # æ·»åŠ è©³ç´°æ—¥èªŒä»¥ä¾¿èª¿è©¦
    print(f"ğŸ” [è·¯ç”±äº‹ä»¶] {event_name}")
    
    # æ¨ç†äº‹ä»¶ â†’ éœ€æ±‚åˆ†æéšæ®µï¼ˆæ€è€ƒä¸­ï¼‰
    if event_name in {
        "ReasoningStarted", "TeamReasoningStarted",
        "ReasoningStep", "TeamReasoningStep",
        "ReasoningContentDelta", "TeamReasoningContentDelta"
    }:
        step_id = "reasoning-thinking"
        routing_state.setdefault(step_id, step_id)
        print(f"ğŸ§  [æ¨ç†æ›´æ–°] LLM æ­£åœ¨æ€è€ƒä¸­...")
        return {"id": step_id, "label": "AI æ€è€ƒä¸­", "status": "running", "eta": "åˆ†ææŒ‡ç¤º...", "stage": "analyze"}

    # TeamRunContent æˆ– RunContent äº‹ä»¶ â†’ æœå°‹è³‡æ–™éšæ®µ
    if event_name in {"TeamRunContent", "RunContent"}:
        step_id = "content-generation"
        routing_state.setdefault(step_id, step_id)
        print(f"âœ… [è·¯ç”±æ›´æ–°] é–‹å§‹ç”Ÿæˆå…§å®¹ â†’ æœå°‹è³‡æ–™éšæ®µ")
        return {"id": step_id, "label": "å…§å®¹ç”Ÿæˆ", "status": "running", "eta": "é€²è¡Œä¸­", "stage": "search"}

    # TeamRunContentCompleted æˆ– RunContentCompleted æˆ– TeamRunCompleted æˆ– RunCompleted â†’ è™•ç†å…§å®¹éšæ®µï¼ˆè—è‰² runningï¼‰
    # é€™äº›äº‹ä»¶è¡¨ç¤º AI ç”Ÿæˆå®Œæˆï¼Œä½†å¾Œç«¯é‚„åœ¨è™•ç†ï¼ˆè§£ææ–°èã€å„²å­˜åˆ°è³‡æ–™åº«ç­‰ï¼‰
    if event_name in {"TeamRunContentCompleted", "RunContentCompleted", "TeamRunCompleted", "RunCompleted"}:
        step_id = "content-processing"
        routing_state.setdefault(step_id, step_id)
        print(f"âœ… [è·¯ç”±æ›´æ–°] {event_name} â†’ è™•ç†å…§å®¹éšæ®µï¼ˆè—è‰²ï¼Œæ­£åœ¨å„²å­˜æ–°èï¼‰")
        return {"id": step_id, "label": "è™•ç†å…§å®¹", "status": "running", "eta": "é€²è¡Œä¸­", "stage": "process"}

    if event_name in {TeamRunEvent.run_started.value, RunEvent.run_started.value}:
        step_id = "run-main"
        routing_state.setdefault(step_id, step_id)
        print(f"âœ… [è·¯ç”±æ›´æ–°] æ¨¡å‹ç”Ÿæˆé–‹å§‹")
        return {"id": step_id, "label": "æ¨¡å‹ç”Ÿæˆ", "status": "running", "eta": "é€²è¡Œä¸­", "stage": "analyze"}

    # run_completed å·²ç¶“åœ¨ä¸Šé¢çš„ Completed äº‹ä»¶ä¸­è™•ç†ï¼Œé€™è£¡ç§»é™¤é‡è¤‡è™•ç†
    # if event_name in {TeamRunEvent.run_completed.value, RunEvent.run_completed.value}:
    #     å·²ç¶“åœ¨ä¸Šé¢çµ±ä¸€è™•ç†ç‚ºã€Œè™•ç†å…§å®¹ã€éšæ®µ

    if event_name in {TeamRunEvent.run_error.value, RunEvent.run_error.value}:
        step_id = "run-main"
        routing_state.setdefault(step_id, step_id)
        return {"id": step_id, "label": "æ¨¡å‹ç”Ÿæˆ", "status": "done", "eta": "å¤±æ•—"}

    if event_name in {TeamRunEvent.tool_call_started.value, RunEvent.tool_call_started.value}:
        tool = getattr(event, "tool", None)
        tool_name = getattr(tool, "tool_name", None)
        tool_key = getattr(tool, "tool_call_id", None)
        if not tool_key:
            tool_key = f"{tool_name or 'tool'}-{getattr(tool, 'created_at', '')}"
        routing_state.setdefault(tool_key, tool_key)
        label = format_tool_label(tool_name)
        print(f"âœ… [è·¯ç”±æ›´æ–°] å·¥å…·èª¿ç”¨é–‹å§‹: {label}")
        return {
            "id": routing_state[tool_key],
            "label": label,
            "status": "running",
            "eta": "é€²è¡Œä¸­",
            "stage": "search",  # å·¥å…·èª¿ç”¨ä¹Ÿç®—åœ¨æœå°‹è³‡æ–™éšæ®µ
        }

    if event_name in {TeamRunEvent.tool_call_completed.value, RunEvent.tool_call_completed.value}:
        tool = getattr(event, "tool", None)
        tool_name = getattr(tool, "tool_name", None)
        tool_key = getattr(tool, "tool_call_id", None)
        if not tool_key:
            tool_key = f"{tool_name or 'tool'}-{getattr(tool, 'created_at', '')}"
        routing_state.setdefault(tool_key, tool_key)
        label = format_tool_label(tool_name)
        print(f"âœ… [è·¯ç”±æ›´æ–°] å·¥å…·èª¿ç”¨å®Œæˆ: {label}")
        return {
            "id": routing_state[tool_key],
            "label": label,
            "status": "done",
            "eta": "",
        }

    if event_name in {TeamRunEvent.tool_call_error.value, RunEvent.tool_call_error.value}:
        tool = getattr(event, "tool", None)
        tool_key = getattr(tool, "tool_call_id", None)
        if not tool_key:
            tool_name = getattr(tool, "tool_name", None) or "tool"
            tool_key = f"{tool_name}-{getattr(tool, 'created_at', '')}"
        routing_state.setdefault(tool_key, tool_key)
        return {
            "id": routing_state[tool_key],
            "label": format_tool_label(getattr(tool, "tool_name", None)),
            "status": "done",
            "eta": "å¤±æ•—",
        }

    return None


def update_routing_log(
    routing_log: List[Dict[str, str]], update: Dict[str, str]
) -> bool:
    for idx, step in enumerate(routing_log):
        if step.get("id") == update.get("id"):
            merged = {**step, **update}
            if merged == step:
                return False
            routing_log[idx] = merged
            return True
    routing_log.append(update)
    return True


def truncate_text(value: Any, max_len: int) -> str:
    if value is None:
        return ""
    if isinstance(value, str):
        text = value.strip()
    else:
        try:
            text = json.dumps(value, ensure_ascii=False)
        except TypeError:
            text = str(value)
    if len(text) <= max_len:
        return text
    return text[:max_len] + "..."


def should_emit_trace_content(text: str) -> bool:
    stripped = text.strip()
    if not stripped:
        return False
    if stripped.startswith("{") or stripped.startswith("["):
        return False
    return True


def format_reasoning_steps(steps: Any) -> str:
    if not steps:
        return ""
    lines = []
    for idx, step in enumerate(steps, start=1):
        title = getattr(step, "title", None) if not isinstance(step, dict) else step.get("title")
        action = getattr(step, "action", None) if not isinstance(step, dict) else step.get("action")
        result = getattr(step, "result", None) if not isinstance(step, dict) else step.get("result")
        parts = [part for part in (title, action, result) if part]
        if not parts:
            continue
        lines.append(f"{idx}. " + " | ".join(parts))
    return "\n".join(lines)


def map_event_to_trace_event(event: Any) -> Optional[Dict[str, Any]]:
    """å°‡ Agno event è½‰æ›ç‚º trace event ä»¥ä¾›å‰ç«¯é¡¯ç¤º"""
    if not isinstance(event, (RunEvent, TeamRunEvent)):
        return None
    
    event_type = getattr(event, "event", None)
    if not event_type:
        return None
    
    # æ•æ‰å·¥å…·èª¿ç”¨äº‹ä»¶ï¼ˆç‰¹åˆ¥æ˜¯ web_searchï¼‰
    if event_type == "tool_call_started":
        tool_name = getattr(event, "tool_name", None)
        tool_args = getattr(event, "tool_arguments", {})
        if tool_name == "web_search_preview":
            query = tool_args.get("query", "")
            return {
                "type": "tool_call",
                "tool": "web_search",
                "message": f"[SEARCH] æœå°‹ä¸­: {query}",
                "args": tool_args,
            }
        return {
            "type": "tool_call",
            "tool": tool_name,
            "message": f"[TOOL] èª¿ç”¨å·¥å…·: {tool_name}",
        }
    
    # æ•æ‰å·¥å…·èª¿ç”¨çµæœ
    if event_type == "tool_call_completed":
        tool_name = getattr(event, "tool_name", None)
        if tool_name == "web_search_preview":
            return {
                "type": "tool_result",
                "tool": "web_search",
                "message": "[OK] æœå°‹å®Œæˆ",
            }
    
    # æ•æ‰ä»£ç†å§”æ´¾äº‹ä»¶
    if event_type == "agent_delegated":
        agent_name = getattr(event, "agent_name", "Agent")
        return {
            "type": "delegation",
            "message": f"[DELEGATE] å§”æ´¾çµ¦: {agent_name}",
        }
    
    return None


def iter_stream_chunks(response: Any) -> Iterator[str]:
    saw_delta = False
    for event in response:
        delta = extract_stream_text(event)
        if delta:
            saw_delta = True
            yield delta
            continue
        if hasattr(event, "get_content_as_string") and not saw_delta:
            content = event.get_content_as_string()
            if content:
                yield content


def ensure_inline_documents_indexed(documents: List[Document]) -> None:
    for doc in documents:
        content = (doc.content or "").strip()
        if not content:
            continue
        if not doc.id:
            doc.id = str(uuid.uuid4())
        name = doc.name or doc.id
        get_rag_store().index_inline_text(doc.id, name, content, doc.type or "TEXT")


def build_rag_agent(doc_ids: List[str], model: OpenAIChat) -> Agent:
    def knowledge_retriever(
        query: str,
        num_documents: Optional[int] = None,
        filters: Optional[Dict[str, Any]] = None,
        dependencies: Optional[Dict[str, Any]] = None,
        **_: Any,
    ) -> Optional[List[Dict[str, Any]]]:
        ids: Optional[List[str]] = None
        if isinstance(filters, dict) and filters.get("doc_ids"):
            ids = filters.get("doc_ids")
        if dependencies and dependencies.get("doc_ids"):
            ids = dependencies.get("doc_ids")
        if not ids:
            ids = doc_ids
        return get_rag_store().search(query, doc_ids=ids, top_k=num_documents or 5)

    return Agent(
        name="RAG Agent",
        role="æ–‡ä»¶æª¢ç´¢èˆ‡è§£æ",
        model=model,
        instructions=RAG_AGENT_INSTRUCTIONS,
        knowledge_retriever=knowledge_retriever,
        search_knowledge=True,
        add_knowledge_to_context=True,
        markdown=False,
    )


def build_research_agent() -> Agent:
    """å»ºç«‹ Deep Research Agentï¼Œå°ˆé–€åŸ·è¡Œæ±å—äºæ–°èæœå°‹"""
    model = get_model(enable_web_search=True, model_id=get_research_model_id())
    
    # æ§‹å»ºæŒ‰å€åŸŸåˆ†çµ„çš„ site: èªæ³•æŸ¥è©¢æ¨¡æ¿
    region_site_queries = {}
    for src in TRUSTED_NEWS_SOURCES:
        region = src["region"]
        if region not in region_site_queries:
            region_site_queries[region] = []
        region_site_queries[region].append(f"site:{src['domain']}")
    
    # æ§‹å»ºæ¯å€‹å€åŸŸçš„å®Œæ•´ site: OR æŸ¥è©¢
    region_queries = {}
    for region, sites in region_site_queries.items():
        region_queries[region] = " OR ".join(sites)
    
    # æ§‹å»ºæŒ‡ä»¤æ–‡å­—
    query_templates = "\n".join([
        f"  - {region}: ({sites})"
        for region, sites in region_queries.items()
    ])
    
    return Agent(
        name="Deep Research Agent",
        role="æ±å—äºæ–°èæ·±åº¦æœå°‹å°ˆå“¡",
        model=model,
        instructions=[
            "ä½ æ˜¯æ±å—äºæ–°èæœå°‹å°ˆå“¡ï¼Œè² è²¬ä½¿ç”¨ web_search å·¥å…·æœå°‹æ±å—äºå„åœ‹æ–°èã€‚",
            "",
            "ã€æ ¸å¿ƒè¦å‰‡ - å¿…é ˆéµå®ˆã€‘",
            "[WARNING] æ¯æ¬¡æœå°‹éƒ½å¿…é ˆä½¿ç”¨ site: èªæ³•é™å®šä¿¡ä»»ç¶²åŸŸï¼Œçµ•å°ä¸å¯çœç•¥ï¼",
            "[WARNING] æœå°‹æŸ¥è©¢æ ¼å¼ï¼š<é—œéµå­—> <siteèªæ³•> <æ™‚é–“é™åˆ¶>",
            "",
            "ã€ä¿¡ä»»ç¶²åŸŸæŸ¥è©¢æ¨¡æ¿ - ç›´æ¥è¤‡è£½ä½¿ç”¨ã€‘",
            query_templates,
            "",
            "ã€æœå°‹æ­¥é©Ÿã€‘",
            "1. è­˜åˆ¥ä½¿ç”¨è€…è¦æŸ¥è©¢çš„å€åŸŸï¼ˆVietnam/Thailand/Singapore/Cambodiaç­‰ï¼‰",
            "2. å¾ä¸Šæ–¹æ¨¡æ¿è¤‡è£½å°æ‡‰å€åŸŸçš„å®Œæ•´ site: èªæ³•",
            "3. çµ„åˆå®Œæ•´æŸ¥è©¢ï¼š<ä½¿ç”¨è€…é—œéµå­—> <siteèªæ³•> after:<æ—¥æœŸ>",
            "4. ä½¿ç”¨ web_search å·¥å…·åŸ·è¡Œæœå°‹",
            "",
            "ã€æ­£ç¢ºæŸ¥è©¢ç¯„ä¾‹ã€‘",
            "âœ… Vietnam fintech (site:viet-jo.com OR site:cafef.vn OR site:vnexpress.net OR site:vietnamfinance.vn OR site:vir.com.vn OR site:vietnambiz.vn OR site:tapchikinhtetaichinh.vn) after:2025-12-20",
            "âœ… Singaporeå¤®è¡Œæ”¿ç­– (site:fintechnews.sg) after:2025-12-01",
            "âœ… Thailandæ•¸ä½æ”¯ä»˜ (site:bangkokpost.com OR site:techsauce.co) after:2025-12-15",
            "",
            "ã€éŒ¯èª¤æŸ¥è©¢ç¯„ä¾‹ - ç¦æ­¢ä½¿ç”¨ã€‘",
            "âŒ Vietnam fintech news  (ç¼ºå°‘ site: èªæ³•)",
            "âŒ fintech site:google.com  (ä½¿ç”¨äº†éä¿¡ä»»ç¶²åŸŸ)",
            "âŒ Singapore news  (æ²’æœ‰é™å®šç¶²åŸŸ)",
            "",
            "ã€è¼¸å‡ºæ ¼å¼ - Markdown æ–°èåˆ—è¡¨ã€‘",
            "å¿…é ˆä»¥ Markdown æ ¼å¼è¼¸å‡ºï¼Œæ¯å‰‡æ–°èåŒ…å«ï¼š",
            "- æ¨™é¡Œï¼ˆä½¿ç”¨ ### æ¨™è¨˜ï¼‰",
            "- ç™¼å¸ƒæ™‚é–“ï¼ˆæ ¼å¼ï¼šYYYY-MM-DD æˆ– YYYYå¹´MMæœˆDDæ—¥ï¼‰",
            "- æ–°èæ‘˜è¦ï¼ˆ1-3 æ®µç°¡æ½”èªªæ˜ï¼‰",
            "- æ–°èä¾†æºé€£çµï¼ˆå®Œæ•´ URLï¼‰",
            "- æ¯å‰‡æ–°èä¹‹é–“ç”¨ç©ºè¡Œåˆ†éš”",
            "",
            "ç¯„ä¾‹è¼¸å‡ºï¼š",
            "### è¶Šå—å¤®è¡Œå®£å¸ƒé™æ¯ 0.5 å€‹ç™¾åˆ†é»",
            "ç™¼å¸ƒæ™‚é–“ï¼š2025-12-28",
            "è¶Šå—åœ‹å®¶éŠ€è¡Œï¼ˆSBVï¼‰ä»Šæ—¥å®£å¸ƒå°‡åŸºæº–åˆ©ç‡ä¸‹èª¿ 0.5 å€‹ç™¾åˆ†é»è‡³ 4.5%ï¼Œé€™æ˜¯ä»Šå¹´ç¬¬ä¸‰æ¬¡é™æ¯ã€‚æ­¤èˆ‰æ—¨åœ¨åˆºæ¿€ç¶“æ¿Ÿæˆé•·ä¸¦æ”¯æŒä¼æ¥­èè³‡ã€‚",
            "https://vnexpress.net/economy/example-url",
            "",
            "### æ³°åœ‹é€šéæ–°æŠ•è³‡ä¿ƒé€²æ³•æ¡ˆ",
            "ç™¼å¸ƒæ™‚é–“ï¼š2025-12-27",
            "æ³°åœ‹å…§é–£æ‰¹å‡†æ–°çš„æŠ•è³‡ä¿ƒé€²æ³•æ¡ˆï¼Œç‚ºå¤–åœ‹æŠ•è³‡è€…æä¾›æœ€é«˜ 8 å¹´çš„ç¨…æ”¶å„ªæƒ ã€‚é‡é»ç”¢æ¥­åŒ…æ‹¬é›»å‹•è»Šã€æ•¸ä½ç¶“æ¿Ÿå’Œç”Ÿç‰©ç§‘æŠ€ã€‚",
            "https://bangkokpost.com/business/example-url",
            "",
            "ã€é‡è¦æé†’ã€‘",
            "- å¿…é ˆè¼¸å‡º Markdown æ ¼å¼ï¼Œä¸è¦ä½¿ç”¨ JSON",
            "- æ¯å‰‡æ–°èéƒ½è¦åŒ…å«å®Œæ•´çš„ URL é€£çµ",
            "- çµ•å°ä¸å¯çœç•¥ site: èªæ³•",
            "- é©—è­‰æ¯å€‹çµæœçš„ç¶²åŸŸæ˜¯å¦åœ¨ä¿¡ä»»æ¸…å–®ä¸­",
            "- è‹¥æ‰¾ä¸åˆ°ä¿¡ä»»ä¾†æºçš„æ–°èï¼Œå»ºè­°æ“´å¤§æ™‚é–“ç¯„åœæˆ–èª¿æ•´é—œéµå­—",
        ],
        tools=[WEB_SEARCH_TOOL],
        search_knowledge=True,
        add_knowledge_to_context=True,
        markdown=True,  # å•Ÿç”¨ Markdown è¼¸å‡º
    )


def build_vision_agent() -> Agent:
    model = get_model(enable_vision=True)
    return Agent(
        name="Vision Agent",
        role="å½±åƒ/æˆªåœ–ç†è§£èˆ‡OCR",
        model=model,
        instructions=[
            "å°ˆæ³¨æ–¼è§£æä¸Šå‚³çš„æˆªåœ–ã€ç…§ç‰‡æˆ–æ–‡ä»¶åœ–ç‰‡ï¼Œæè¿°é—œéµå…§å®¹èˆ‡æ–‡å­—ã€‚",
            "è‹¥æ²’æœ‰å½±åƒå¯è®€ï¼Œè«‹è¦æ±‚ä½¿ç”¨è€…æä¾›åœ–ç‰‡æˆ–ç¢ºèªæ ¼å¼ã€‚",
        ],
        markdown=False,
    )


def build_team(
    doc_ids: List[str],
    enable_web_search: bool = False,
    enable_vision: bool = False,
) -> Team:
    model = get_model(enable_web_search=enable_web_search, enable_vision=enable_vision)
    # RAG Agent å·²åœç”¨ä»¥æå‡é€Ÿåº¦ï¼Œå¦‚éœ€å•Ÿç”¨è«‹å–æ¶ˆä¸‹æ–¹è¨»è§£
    # rag_agent = build_rag_agent(doc_ids, get_model())
    research_agent = build_research_agent()
    vision_agent = build_vision_agent()
    return Team(
        name="æ±å—äºæ–°èè¼¿æƒ…åˆ†æåŠ©ç†",
        members=[research_agent, vision_agent],  # ç§»é™¤ rag_agent
        model=model,
        instructions=TEAM_INSTRUCTIONS,
        expected_output=EXPECTED_OUTPUT,
        tools=[WEB_SEARCH_TOOL] if enable_web_search else [],
        add_member_tools_to_context=True,
        add_name_to_context=True,
        add_datetime_to_context=True,
        delegate_to_all_members=False,  # Team Leader decides when to delegate
        store_events=STORE_EVENTS,
        markdown=False,
        stream=enable_web_search,  # å•Ÿç”¨ streaming ç•¶ä½¿ç”¨ web search
    )


def safe_parse_json(text: str) -> Dict[str, Any]:
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        start = text.find("{")
        end = text.rfind("}")
        if start != -1 and end != -1 and end > start:
            try:
                return json.loads(text[start : end + 1])
            except json.JSONDecodeError:
                pass
        # Return a fallback response if JSON parsing fails
        return build_empty_response(f"æŠ±æ­‰ï¼Œè™•ç†éç¨‹ä¸­ç™¼ç”Ÿå•é¡Œã€‚åŸå§‹å›æ‡‰ï¼š{text[:200]}...")


app = FastAPI(title="Agno Artifacts API", version="1.1.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"],
)

SSE_HEADERS = {
    "Cache-Control": "no-cache, no-transform",
    "X-Accel-Buffering": "no",
    "Connection": "keep-alive",
    "Transfer-Encoding": "chunked",
}


def preload_sample_pdfs() -> None:
    """é åŠ è¼‰ src/docs ç›®éŒ„ä¸‹çš„ç¤ºä¾‹ PDF æ–‡ä»¶"""
    docs_dir = os.path.join(os.path.dirname(__file__), "..", "src", "docs")
    if not os.path.isdir(docs_dir):
        return

    for filename in os.listdir(docs_dir):
        if not filename.lower().endswith(".pdf"):
            continue
        filepath = os.path.join(docs_dir, filename)
        try:
            with open(filepath, "rb") as f:
                data = f.read()
            get_rag_store().index_pdf_bytes(data, filename)
            print(f"âœ“ é åŠ è¼‰ PDF: {filename}")
        except Exception as exc:
            print(f"âœ— é åŠ è¼‰ PDF å¤±æ•— {filename}: {exc}")


@app.on_event("startup")
async def startup_event():
    """æ‡‰ç”¨å•Ÿå‹•æ™‚çš„åˆå§‹åŒ–ä»»å‹™"""
    # é åŠ è¼‰ç¤ºä¾‹ PDF
    preload_sample_pdfs()
    
    # é…ç½®éœæ…‹æ–‡ä»¶æœå‹™ï¼ˆå¿…é ˆåœ¨æ‰€æœ‰ API è·¯ç”±ä¹‹å¾Œï¼‰
    dist_path = Path(__file__).parent.parent / "dist"
    if dist_path.exists() and dist_path.is_dir():
        try:
            # æª¢æŸ¥ API è·¯ç”±æ•¸é‡
            api_routes = [r for r in app.routes if hasattr(r, 'path') and r.path.startswith('/api')]
            print(f"[OK] æª¢æ¸¬åˆ° {len(api_routes)} å€‹ API è·¯ç”±")
            
            # ä½¿ç”¨ StaticFiles çš„ html=True åƒæ•¸è™•ç† SPA
            app.mount("/", StaticFiles(directory=str(dist_path), html=True), name="static")
            print(f"[OK] éœæ…‹æ–‡ä»¶æœå‹™å·²å•Ÿç”¨ (html=True): {dist_path}")
        except Exception as e:
            print(f"[WARN] æ›è¼‰éœæ…‹æ–‡ä»¶å¤±æ•—: {e}")
    else:
        print("[WARN] è­¦å‘Š: dist ç›®éŒ„ä¸å­˜åœ¨ï¼Œéœæ…‹æ–‡ä»¶æœå‹™æœªå•Ÿç”¨")
        print("   ç”Ÿç”¢ç’°å¢ƒè«‹å…ˆé‹è¡Œ: npm run build")


@app.post("/api/auth/login")
async def login(request: LoginRequest):
    """ç”¨æˆ·ç™»å½•éªŒè¯æ¥å£"""
    try:
        # æ¸…ç†è¿‡æœŸçš„ä¼šè¯
        cleanup_expired_sessions()
        
        # ä»ç¯å¢ƒå˜é‡è¯»å–å‡­è¯
        valid_username = os.getenv("APP_USERNAME", "CathaySEA")
        valid_password = os.getenv("APP_PASSWORD", "CathaySEA")
        
        # éªŒè¯ç”¨æˆ·åå’Œå¯†ç 
        if request.username == valid_username and request.password == valid_password:
            # æ¸…ç©ºæ‰€æœ‰è³‡æ–™ï¼ˆå–®ç”¨æˆ¶æ¨¡å¼ï¼šæ¯æ¬¡ç™»å…¥éƒ½æ˜¯ä¹¾æ·¨ç‹€æ…‹ï¼‰
            print("[ç™»å…¥] æ¸…ç©ºæ‰€æœ‰è³‡æ–™...")
            news_store.clear_all_records()
            clear_all_tags()
            print("[ç™»å…¥] è³‡æ–™æ¸…ç©ºå®Œæˆ")
            
            # ç”Ÿæˆä¼šè¯ä»¤ç‰Œ
            token = create_session_token()
            active_sessions[token] = datetime.now() + SESSION_TIMEOUT
            
            return LoginResponse(
                success=True,
                token=token
            )
        else:
            return LoginResponse(
                success=False,
                error="å¸³è™Ÿæˆ–å¯†ç¢¼éŒ¯èª¤"
            )
    except Exception as e:
        print(f"ç™»å½•é”™è¯¯: {e}")
        return LoginResponse(
            success=False,
            error="ç™»å…¥éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤"
        )


class VerifyTokenRequest(BaseModel):
    token: str


@app.post("/api/auth/verify")
async def verify_token(request: VerifyTokenRequest):
    """éªŒè¯ä¼šè¯ä»¤ç‰Œæ˜¯å¦æœ‰æ•ˆ"""
    try:
        cleanup_expired_sessions()
        is_valid = verify_session(request.token)
        return {"valid": is_valid}
    except Exception as e:
        print(f"éªŒè¯é”™è¯¯: {e}")
        return {"valid": False}


@app.post("/api/auth/clear-data")
async def clear_user_data():
    """æ¸…ç©ºæ‰€æœ‰ç”¨æˆ¶è³‡æ–™ï¼ˆç”¨æ–¼ç™»å…¥æ™‚ï¼‰"""
    try:
        print("[API] æ¸…ç©ºæ‰€æœ‰ç”¨æˆ¶è³‡æ–™...")
        news_store.clear_all_records()
        clear_all_tags()
        print("[API] è³‡æ–™æ¸…ç©ºå®Œæˆ")
        return {"success": True}
    except Exception as e:
        print(f"æ¸…ç©ºè³‡æ–™éŒ¯èª¤: {e}")
        return {"success": False, "error": str(e)}


@app.get("/api/health")
async def health():
    return {"ok": True}


@app.get("/api/tags")
async def get_tags():
    store = load_tag_store()
    return {
        "custom_tags": store.get("custom_tags", []),
        "doc_tags": store.get("docs", {}),
    }


@app.post("/api/tags")
async def update_tags(req: TagUpdateRequest):
    if req.tag_key and req.tags is not None:
        set_doc_tags(req.tag_key, req.tags)
    if req.custom_tags is not None:
        set_custom_tags(req.custom_tags)
    return {"ok": True}


@app.get("/api/documents/preloaded")
async def get_preloaded_documents():
    """ç²å–é åŠ è¼‰çš„æ–‡æª”åˆ—è¡¨"""
    documents = []
    for doc_id, stored in get_rag_store().docs.items():
        tag_key = stored.content_hash or stored.id
        documents.append(
            {
                "id": stored.id,
                "name": stored.name,
                "type": stored.type,
                "pages": stored.pages or "-",
                "tags": get_doc_tags(tag_key),
                "tag_key": tag_key,
                "status": stored.status,
                "message": stored.message,
                "preview": stored.preview,
            }
        )
    return {"documents": documents}


@app.post("/api/documents")
async def upload_documents(files: List[UploadFile] = File(...)):
    if not files:
        return JSONResponse({"error": "No files provided"}, status_code=400)

    results = []
    for file in files:
        filename = file.filename or f"upload-{uuid.uuid4()}"
        ext = os.path.splitext(filename)[1].lower()
        data = await file.read()
        tag_key = compute_tag_key(data)
        stored_tags = get_doc_tags(tag_key)

        try:
            if ext == ".pdf":
                stored = get_rag_store().index_pdf_bytes(data, filename)
            elif ext in {".txt", ".md", ".csv"}:
                stored = get_rag_store().index_text_bytes(data, filename)
            elif ext in IMAGE_EXTENSIONS:
                doc_id = str(uuid.uuid4())
                mime_type, _ = guess_type(filename)
                mime_type = mime_type or f"image/{ext.lstrip('.')}"
                image_payload = base64.b64encode(data).decode("utf-8")
                results.append(
                    {
                        "id": doc_id,
                        "name": os.path.splitext(filename)[0],
                        "type": ext.upper().lstrip("."),
                        "pages": "-",
                        "tags": stored_tags,
                        "tag_key": tag_key,
                        "status": "indexed",
                        "message": "",
                        "preview": "",
                        "image": f"data:{mime_type};base64,{image_payload}",
                        "image_mime": mime_type,
                    }
                )
                continue
            else:
                stored = get_rag_store().register_stub(filename, ext.upper().lstrip(".") or "FILE", "å°šæœªæ”¯æ´æ­¤æ ¼å¼")
        except Exception as exc:
            stored = get_rag_store().register_stub(filename, ext.upper().lstrip(".") or "FILE", str(exc))

        results.append(
            {
                "id": stored.id,
                "name": stored.name,
                "type": stored.type,
                "pages": stored.pages or "-",
                "tags": stored_tags,
                "tag_key": tag_key,
                "status": stored.status,
                "message": stored.message,
                "preview": stored.preview,
            }
        )

    return {"documents": results}


@app.get("/api/documents/preloaded")
async def get_preloaded_documents():
    docs_dir = Path(__file__).resolve().parent.parent / "src" / "docs"
    if not docs_dir.exists():
        return {"documents": []}

    results = []
    for file_path in docs_dir.glob("*.pdf"):
        try:
            data = file_path.read_bytes()
            tag_key = compute_tag_key(data)
            stored = get_rag_store().index_pdf_bytes(data, file_path.name)
            results.append(
                {
                    "id": stored.id,
                    "name": stored.name,
                    "type": stored.type,
                    "pages": stored.pages or "-",
                    "tags": get_doc_tags(tag_key),
                    "tag_key": tag_key,
                    "status": stored.status,
                    "message": stored.message,
                    "preview": stored.preview,
                }
            )
        except Exception as exc:
            results.append(
                {
                    "id": str(uuid.uuid4()),
                    "name": file_path.stem,
                    "type": "PDF",
                    "pages": "-",
                    "tags": [],
                    "tag_key": "",
                    "status": "failed",
                    "message": str(exc),
                    "preview": "",
                }
            )

    return {"documents": results}


@app.post("/api/artifacts")
async def generate_artifacts(req: ArtifactRequest):
    try:
        import time
        start_time = time.time()
        
        last_user = get_last_user_message(req.messages)
        
        print(f"â±ï¸ [è¨ˆæ™‚] é–‹å§‹è·¯ç”±åˆ¤æ–·")
        route = run_router_agent(req.messages, req.documents, req.system_context)
        route_time = time.time() - start_time
        print(f"â±ï¸ [è¨ˆæ™‚] è·¯ç”±åˆ¤æ–·å®Œæˆï¼Œè€—æ™‚: {route_time:.2f}ç§’, çµæœ: {route}")
        
        if route and route.mode == "simple":
            # Return SSE format if streaming is requested
            if req.stream:
                agent = build_smalltalk_agent(req.documents, req.system_context)
                smalltalk_prompt = build_smalltalk_prompt(req.messages)
                response = agent.run(smalltalk_prompt or "ä½ å¥½", stream=True, stream_events=True)

                async def generate_smalltalk_sse():
                    accumulated = ""
                    reasoning_fragments: List[str] = []
                    try:
                        routing_update = {
                            "id": "run-main",
                            "label": "æ¨¡å‹ç”Ÿæˆ",
                            "status": "running",
                            "eta": "é€²è¡Œä¸­",
                        }
                        yield f"data: {json.dumps({'routing_update': routing_update})}\n\n"
                        for event in response:
                            trace_event = map_event_to_trace_event(event)
                            if trace_event:
                                yield f"data: {json.dumps({'trace_event': trace_event})}\n\n"

                            reasoning_text = extract_reasoning_text(event)
                            if reasoning_text:
                                reasoning_fragments.append(reasoning_text)

                            content = extract_stream_text(event)
                            if not content:
                                continue
                            accumulated += content
                            yield f"data: {json.dumps({'chunk': content})}\n\n"

                        final_data = build_empty_response(
                            accumulated
                            or "ä½ å¥½ï¼æˆ‘æ˜¯æˆä¿¡å ±å‘ŠåŠ©ç†ï¼Œå¯ä»¥å”åŠ©æ‘˜è¦ã€ç¿»è­¯ã€é¢¨éšªè©•ä¼°èˆ‡æˆä¿¡å ±å‘Šè‰ç¨¿ã€‚"
                        )
                        final_data["routing"] = [
                            {
                                "id": "run-main",
                                "label": "æ¨¡å‹ç”Ÿæˆ",
                                "status": "done",
                                "eta": "å®Œæˆ",
                            }
                        ]
                        if reasoning_fragments:
                            final_data["reasoning_summary"] = build_reasoning_summary(reasoning_fragments)
                        yield f"data: {json.dumps(final_data)}\n\n"
                    except Exception as exc:
                        error_response = build_empty_response(f"è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{str(exc)}")
                        yield f"data: {json.dumps(error_response)}\n\n"
                    yield f"data: {json.dumps({'done': True})}\n\n"

                return StreamingResponse(
                    generate_smalltalk_sse(),
                    media_type="text/event-stream",
                    headers=SSE_HEADERS,
                )

            reply = run_smalltalk_agent(
                req.messages, req.documents, req.system_context
            )
            response_data = build_empty_response(reply)
            return response_data

        convo = build_conversation(req.messages)
        image_inputs = build_image_inputs(req.documents)

        # Add system status to prompt for Team
        system_status = build_system_status(req.documents, req.system_context)
        use_web_search = bool(route and route.needs_web_search)
        use_rag = bool(route and route.needs_rag)
        use_vision = bool(route and route.needs_vision) or bool(image_inputs)
        if req.stream:
            async def generate_sse():
                import time
                timings = {
                    "request_start": time.time(),
                    "team_built": None,
                    "first_event": None,
                    "web_search_start": None,
                    "web_search_end": None,
                    "first_content": None,
                    "done": None,
                }
                accumulated = ""
                assistant_content_len = 0
                streamed_news_keys: Set[str] = set()
                routing_state: Dict[str, str] = {}
                routing_log: List[Dict[str, str]] = []
                ocr_updates: List[Dict[str, Any]] = []
                reasoning_fragments: List[str] = []

                try:
                    if image_inputs:
                        ocr_start = {
                            "id": "ocr",
                            "label": "OCR è§£æ",
                            "status": "running",
                            "eta": "é€²è¡Œä¸­",
                        }
                        if update_routing_log(routing_log, ocr_start):
                            yield f"data: {json.dumps({'routing_update': ocr_start})}\n\n"
                        ocr_updates = run_ocr_for_documents(req.documents)
                        ocr_done = {
                            "id": "ocr",
                            "label": "OCR è§£æ",
                            "status": "done",
                            "eta": "å®Œæˆ",
                        }
                        if update_routing_log(routing_log, ocr_done):
                            yield f"data: {json.dumps({'routing_update': ocr_done})}\n\n"

                    # RAG ç´¢å¼•å·²åœç”¨ä»¥æå‡é€Ÿåº¦ï¼Œå¦‚éœ€å•Ÿç”¨è«‹å–æ¶ˆä¸‹æ–¹è¨»è§£
                    # ensure_inline_documents_indexed(req.documents)
                    doc_ids = []
                    # doc_ids = [
                    #     doc.id
                    #     for doc in req.documents
                    #     if doc.id and doc.id in rag_store.docs
                    # ]

                    team = build_team(
                        doc_ids,
                        enable_web_search=use_web_search,
                        enable_vision=use_vision,
                    )
                    timings["team_built"] = time.time()
                    print(f"â±ï¸ [è¨ˆæ™‚] Team å»ºç«‹å®Œæˆ: {timings['team_built'] - timings['request_start']:.2f}s")

                    if use_web_search:
                        team.tool_choice = WEB_SEARCH_TOOL


                    doc_context = build_doc_context(
                        req.documents,
                        req.system_context.selected_doc_id if req.system_context else None,
                        include_content=not use_web_search or use_rag or use_vision,
                    )
                    prompt = f"{convo}\n\n{system_status}\n\n{doc_context}\n\nè«‹ä¾è¦å‰‡ç”¢å‡º JSONã€‚"

                    run_start = {
                        "id": "run-main",
                        "label": "æ¨¡å‹ç”Ÿæˆ",
                        "status": "running",
                        "eta": "é€²è¡Œä¸­",
                    }
                    if update_routing_log(routing_log, run_start):
                        yield f"data: {json.dumps({'routing_update': run_start})}\n\n"

                    response = team.run(
                        prompt,
                        # dependencies={"doc_ids": doc_ids},  # RAG å·²åœç”¨
                        # add_dependencies_to_context=True,
                        images=image_inputs if image_inputs else None,
                        stream=True,
                        stream_events=True,
                    )

                    for event in response:
                        # è™•ç†è·¯ç”±æ›´æ–° - å³æ™‚ç™¼é€çµ¦å‰ç«¯
                        routing_update = build_routing_update(event, routing_state)
                        if routing_update:
                            log_line = f"ğŸ”§ [è·¯ç”±å»ºç«‹] ç”¢ç”Ÿæ›´æ–°ç‰©ä»¶: {routing_update}"
                            print(log_line)
                            yield f"data: {json.dumps({'log_chunk': log_line})}\n\n"
                            
                            should_send = update_routing_log(routing_log, routing_update)
                            log_line = f"ğŸ” [å»é‡æª¢æŸ¥] æ˜¯å¦ç™¼é€: {should_send}"
                            print(log_line)
                            yield f"data: {json.dumps({'log_chunk': log_line})}\n\n"
                            
                            if should_send:
                                log_line = f"ğŸ“¤ [å³æ™‚æ¨é€] è·¯ç”±æ›´æ–°: {routing_update}"
                                print(log_line)
                                yield f"data: {json.dumps({'log_chunk': log_line})}\n\n"
                                yield f"data: {json.dumps({'routing_update': routing_update})}\n\n"

                        # æ¨é€äº‹ä»¶åç¨±æ—¥èªŒ
                        event_name = getattr(event, "event", "") or ""
                        if event_name:
                            log_line = f"ğŸ” [è·¯ç”±äº‹ä»¶] {event_name}"
                            print(log_line)
                            yield f"data: {json.dumps({'log_chunk': log_line})}\n\n"

                        # æå–æ¨ç†éç¨‹ï¼ˆå¦‚æœæœ‰ï¼‰
                        reasoning_text = extract_reasoning_text(event)
                        if reasoning_text:
                            reasoning_fragments.append(reasoning_text)
                            log_line = f"ğŸ§  [æ¨ç†æ—¥èªŒ] {reasoning_text[:200]}..."
                            print(log_line)
                            yield f"data: {json.dumps({'log_chunk': log_line})}\n\n"

                        trace_event = map_event_to_trace_event(event)
                        if trace_event:
                            # è¨˜éŒ„ web_search æ™‚é–“
                            if trace_event.get("tool") == "web_search":
                                if trace_event.get("type") == "tool_call" and not timings["web_search_start"]:
                                    timings["web_search_start"] = time.time()
                                    elapsed = timings["web_search_start"] - timings["request_start"]
                                    print(f"â±ï¸ [è¨ˆæ™‚] Web Search é–‹å§‹: {elapsed:.2f}s")
                                elif trace_event.get("type") != "tool_call" and not timings["web_search_end"]:
                                    timings["web_search_end"] = time.time()
                                    search_duration = timings["web_search_end"] - (timings["web_search_start"] or timings["request_start"])
                                    print(f"â±ï¸ [è¨ˆæ™‚] Web Search å®Œæˆ: è€—æ™‚ {search_duration:.2f}s")
                                
                                search_status = "running" if trace_event.get("type") == "tool_call" else "done"
                                search_label = trace_event.get("message", "ç¶²é æœå°‹ä¸­...")
                                web_search_update = {
                                    "id": "web-search",
                                    "label": search_label,
                                    "status": search_status,
                                    "eta": "æœå°‹é€²è¡Œä¸­" if search_status == "running" else "å®Œæˆ",
                                    "stage": "searching" if search_status == "running" else "complete",
                                }
                                yield f"data: {json.dumps({'routing_update': web_search_update})}\n\n"
                            else:
                                yield f"data: {json.dumps({'trace_event': trace_event})}\n\n"

                        content = extract_stream_text(event)
                        if not content:
                            continue
                        # è¨˜éŒ„ç¬¬ä¸€å€‹å…§å®¹æ™‚é–“
                        if not timings["first_content"]:
                            timings["first_content"] = time.time()
                            elapsed = timings["first_content"] - timings["request_start"]
                            print(f"â±ï¸ [è¨ˆæ™‚] é¦–æ¬¡å…§å®¹è¼¸å‡º: {elapsed:.2f}s")
                        accumulated += content
                        yield f"data: {json.dumps({'chunk': content})}\n\n"

                        if use_web_search:
                            assistant_content = extract_assistant_content_from_json(accumulated)
                            if assistant_content and len(assistant_content) > assistant_content_len:
                                assistant_content_len = len(assistant_content)
                                articles = parse_news_articles_streaming(assistant_content)
                                new_docs = build_news_records_from_articles(
                                    articles,
                                    seen_keys=streamed_news_keys,
                                )
                                if new_docs:
                                    yield f"data: {json.dumps({'documents_append': new_docs})}\n\n"


                    run_done = {
                        "id": "run-main",
                        "label": "æ¨¡å‹ç”Ÿæˆ",
                        "status": "done",
                        "eta": "å®Œæˆ",
                    }
                    if update_routing_log(routing_log, run_done):
                        yield f"data: {json.dumps({'routing_update': run_done})}\n\n"

                    # Parse and send final complete message
                    if accumulated:
                        final_data = safe_parse_json(accumulated)
                        if routing_log:
                            final_data["routing"] = routing_log
                        if ocr_updates:
                            final_data["documents_update"] = ocr_updates
                        reasoning_summary = build_reasoning_summary(reasoning_fragments)
                        if reasoning_summary:
                            final_data["reasoning_summary"] = reasoning_summary
                        news_docs = build_news_documents(
                            final_data,
                            last_user,
                            use_web_search,
                            seen_keys=streamed_news_keys,
                        )
                        if news_docs:
                            existing_docs = final_data.get("documents_append") or []
                            final_data["documents_append"] = existing_docs + news_docs
                        yield f"data: {json.dumps(final_data)}\n\n"
                    else:
                        # No content accumulated, send fallback response
                        fallback = build_empty_response("æŠ±æ­‰ï¼Œæˆ‘ç„¡æ³•å®Œæˆé€™å€‹è«‹æ±‚ã€‚è«‹ç¨å¾Œå†è©¦ã€‚")
                        yield f"data: {json.dumps(fallback)}\n\n"
                except Exception as exc:
                    error_response = build_empty_response(f"è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{str(exc)}")
                    yield f"data: {json.dumps(error_response)}\n\n"
                
                # è¼¸å‡ºè¨ˆæ™‚ç¸½çµ
                timings["done"] = time.time()
                total_time = timings["done"] - timings["request_start"]
                print("\n" + "="*60)
                print("â±ï¸ [è¨ˆæ™‚ç¸½çµ]")
                print("="*60)
                if timings["team_built"]:
                    print(f"  Team å»ºç«‹:    {timings['team_built'] - timings['request_start']:.2f}s")
                if timings["web_search_start"]:
                    ws_start = timings["web_search_start"] - timings["request_start"]
                    print(f"  Web Search é–‹å§‹: {ws_start:.2f}s")
                if timings["web_search_end"] and timings["web_search_start"]:
                    ws_duration = timings["web_search_end"] - timings["web_search_start"]
                    print(f"  Web Search è€—æ™‚: {ws_duration:.2f}s âš ï¸")
                if timings["first_content"]:
                    print(f"  é¦–æ¬¡å…§å®¹:     {timings['first_content'] - timings['request_start']:.2f}s")
                print(f"  ç¸½è€—æ™‚:       {total_time:.2f}s")
                print("="*60 + "\n")
                
                yield f"data: {json.dumps({'done': True})}\n\n"


            return StreamingResponse(
                generate_sse(),
                media_type="text/event-stream",
                headers=SSE_HEADERS,
            )
        else:
            # Non-streaming response
            ocr_updates = run_ocr_for_documents(req.documents)
            # RAG ç´¢å¼•å·²åœç”¨ä»¥æå‡é€Ÿåº¦ï¼Œå¦‚éœ€å•Ÿç”¨è«‹å–æ¶ˆä¸‹æ–¹è¨»è§£
            # ensure_inline_documents_indexed(req.documents)
            doc_ids = []
            # doc_ids = [
            #     doc.id
            #     for doc in req.documents
            #     if doc.id and doc.id in rag_store.docs
            # ]
            team = build_team(
                doc_ids,
                enable_web_search=use_web_search,
                enable_vision=use_vision,
            )
            if use_web_search:
                team.tool_choice = WEB_SEARCH_TOOL
            doc_context = build_doc_context(
                req.documents,
                req.system_context.selected_doc_id if req.system_context else None,
                include_content=not use_web_search or use_rag or use_vision,
            )
            prompt = f"{convo}\n\n{system_status}\n\n{doc_context}\n\nè«‹ä¾è¦å‰‡ç”¢å‡º JSONã€‚"
            response = team.run(
                prompt,
                dependencies={"doc_ids": doc_ids},
                add_dependencies_to_context=True,
                images=image_inputs if image_inputs else None,
            )
            text = response.get_content_as_string()
            data: Dict[str, Any] = safe_parse_json(text)
            # Attach reasoning summary if available on the response object
            reasoning_payload = getattr(response, "reasoning", None)
            reasoning_summary = ""
            if isinstance(reasoning_payload, dict):
                reasoning_summary = reasoning_payload.get("summary") or reasoning_payload.get("text") or ""
            if not reasoning_summary:
                reasoning_summary = getattr(response, "reasoning_summary", "") or getattr(response, "reasoning_content", "")
            reasoning_summary = (reasoning_summary or "").strip()
            if reasoning_summary:
                data["reasoning_summary"] = truncate_text(reasoning_summary, TRACE_MAX_LEN)
            if ocr_updates:
                data["documents_update"] = ocr_updates
            news_docs = build_news_documents(data, last_user, use_web_search)
            if news_docs:
                data["documents_append"] = news_docs
            return data
    except Exception as exc:  # noqa: BLE001
        return {
            "error": "LLM request failed",
            "detail": str(exc),
        }


class ExportNewsRequest(BaseModel):
    """åŒ¯å‡ºæ–°èè«‹æ±‚"""
    document_id: str = Field(..., description="æ–‡ä»¶ ID")
    document_name: str = Field(..., description="æ–‡ä»¶åç¨±")
    document_content: str = Field(..., description="æ–‡ä»¶å…§å®¹ï¼ˆåŒ…å«æ–°èåˆ—è¡¨ï¼‰")
    recipient_email: str = Field(..., description="æ”¶ä»¶äººéƒµç®±åœ°å€")
    subject: Optional[str] = Field(default="æ±å—äºæ–°èè¼¿æƒ…å ±å‘Š", description="éƒµä»¶ä¸»æ—¨")


@app.post("/api/export-news")
async def export_and_send_news(req: ExportNewsRequest):
    """
    å¾æ–‡ä»¶å…§å®¹ä¸­è§£ææ–°èåˆ—è¡¨ï¼ŒåŒ¯å‡ºåˆ° Excel ä¸¦ç™¼é€éƒµä»¶
    """
    try:
        if not req.document_content:
            return JSONResponse(
                status_code=400,
                content={"success": False, "error": "æ–‡ä»¶å…§å®¹ç‚ºç©º"}
            )
        
        if not req.recipient_email:
            return JSONResponse(
                status_code=400,
                content={"success": False, "error": "æœªæä¾›æ”¶ä»¶äººéƒµç®±åœ°å€"}
            )
        
        # ä½¿ç”¨çµ•å°è·¯å¾‘ç¢ºä¿æª”æ¡ˆä½ç½®æ­£ç¢º
        base_dir = Path(__file__).parent
        output_dir = base_dir / "exports"
        output_dir.mkdir(exist_ok=True)
        
        print(f"[INFO] è¼¸å‡ºç›®éŒ„: {output_dir}")
        print(f"[INFO] æ–‡ä»¶åç¨±: {req.document_name}")
        print(f"ğŸ“ å…§å®¹é•·åº¦: {len(req.document_content)} å­—å…ƒ")
        
        # ç”Ÿæˆ Excel æª”æ¡ˆï¼ˆå‚³å…¥æ–‡ä»¶å…§å®¹é€²è¡Œè§£æï¼‰
        excel_result = generate_news_excel(
            document_name=req.document_name,
            document_content=req.document_content,
            output_dir=str(output_dir)
        )
        
        if not excel_result.get("success"):
            print(f"[ERROR] Excel ç”Ÿæˆå¤±æ•—: {excel_result.get('error')}")
            return JSONResponse(
                status_code=500,
                content={"success": False, "error": excel_result.get("error", "ç”Ÿæˆ Excel å¤±æ•—")}
            )
        
        filepath = excel_result["filepath"]
        filename = excel_result["filename"]
        news_items = excel_result.get("news_items", [])
        
        print(f"[OK] Excel å·²ç”Ÿæˆ: {filepath}")
        print(f"[INFO] æ–°èæ•¸é‡: {len(news_items)}")
        print(f"ğŸ“‚ æª”æ¡ˆå­˜åœ¨: {os.path.exists(filepath)}")
        print(f"ğŸ“¦ æª”æ¡ˆå¤§å°: {os.path.getsize(filepath) if os.path.exists(filepath) else 0} bytes")
        
        # ç”Ÿæˆéƒµä»¶å…§å®¹
        email_body = generate_news_report_html(
            document_name=req.document_name,
            news_items=news_items
        )
        
        print(f"ğŸ“§ æº–å‚™ç™¼é€éƒµä»¶è‡³: {req.recipient_email}")
        print(f"ğŸ“ é™„ä»¶è·¯å¾‘: {filepath}")
        print(f"ğŸ“ é™„ä»¶åç¨±: {filename}")
        
        # ç™¼é€éƒµä»¶
        email_result = send_email_with_attachment(
            to_email=req.recipient_email,
            subject=req.subject,
            body=email_body,
            attachment_path=filepath,
            attachment_name=filename
        )
        
        print(f"ğŸ“¬ éƒµä»¶ç™¼é€çµæœ: {email_result}")
        
        # æ¸…ç†èˆŠæª”æ¡ˆï¼ˆä¿ç•™ 7 å¤©ï¼‰
        cleanup_old_exports(output_dir=str(output_dir), max_age_days=7)
        
        if email_result.get("success"):
            return JSONResponse(
                content={
                    "success": True,
                    "message": f"å·²æˆåŠŸåŒ¯å‡º {excel_result['count']} ç­†æ–°èä¸¦ç™¼é€è‡³ {req.recipient_email}",
                    "filename": filename,
                    "count": excel_result["count"]
                }
            )
        else:
            return JSONResponse(
                status_code=500,
                content={
                    "success": False,
                    "error": email_result.get("error", "ç™¼é€éƒµä»¶å¤±æ•—"),
                    "excel_generated": True,
                    "filepath": filepath
                }
            )
    
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"success": False, "error": f"è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}"}
        )


@app.get("/api/news/records")
async def get_news_records():
    """
    ç²å–æ‰€æœ‰æ–°èè¨˜éŒ„
    """
    try:
        records = news_store.get_all_records()
        return JSONResponse(content={"documents": records})
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"ç²å–æ–°èè¨˜éŒ„å¤±æ•—: {str(e)}"}
        )


class BatchExportNewsRequest(BaseModel):
    """æ‰¹æ¬¡åŒ¯å‡ºæ–°èè«‹æ±‚"""
    documents: List[Dict[str, str]] = Field(..., description="æ–‡ä»¶åˆ—è¡¨ï¼Œæ¯å€‹åŒ…å« id, name, content")
    recipient_email: str = Field(..., description="æ”¶ä»¶äººéƒµç®±åœ°å€")
    subject: Optional[str] = Field(default="æ±å—äºæ–°èè¼¿æƒ…å ±å‘Šï¼ˆæ‰¹æ¬¡åŒ¯å‡ºï¼‰", description="éƒµä»¶ä¸»æ—¨")


@app.post("/api/export-news-batch")
async def export_and_send_news_batch(req: BatchExportNewsRequest):
    """
    æ‰¹æ¬¡åŒ¯å‡ºå¤šå€‹æ–‡ä»¶çš„æ–°èåˆ°ä¸€å€‹ Excel ä¸¦ç™¼é€éƒµä»¶
    """
    try:
        if not req.documents or len(req.documents) == 0:
            return JSONResponse(
                status_code=400,
                content={"success": False, "error": "æœªæä¾›æ–‡ä»¶"}
            )
        
        if not req.recipient_email:
            return JSONResponse(
                status_code=400,
                content={"success": False, "error": "æœªæä¾›æ”¶ä»¶äººéƒµç®±åœ°å€"}
            )
        
        # ä½¿ç”¨çµ•å°è·¯å¾‘ç¢ºä¿æª”æ¡ˆä½ç½®æ­£ç¢º
        base_dir = Path(__file__).parent
        output_dir = base_dir / "exports"
        output_dir.mkdir(exist_ok=True)
        
        print(f"[INFO] è¼¸å‡ºç›®éŒ„: {output_dir}")
        print(f"ğŸ“¦ æ–‡ä»¶æ•¸é‡: {len(req.documents)}")
        print(f"ğŸ“ æ–‡ä»¶åˆ—è¡¨: {[doc.get('name', 'æœªå‘½å') for doc in req.documents]}")
        
        # æ‰¹æ¬¡ç”Ÿæˆ Excel æª”æ¡ˆ
        excel_result = generate_batch_news_excel(
            documents=req.documents,
            output_dir=str(output_dir)
        )
        
        if not excel_result.get("success"):
            print(f"[ERROR] Excel æ‰¹æ¬¡ç”Ÿæˆå¤±æ•—: {excel_result.get('error')}")
            return JSONResponse(
                status_code=500,
                content={"success": False, "error": excel_result.get("error", "æ‰¹æ¬¡ç”Ÿæˆ Excel å¤±æ•—")}
            )
        
        filepath = excel_result["filepath"]
        filename = excel_result["filename"]
        news_items = excel_result.get("news_items", [])
        
        print(f"[OK] Excel å·²ç”Ÿæˆ: {filepath}")
        print(f"[INFO] æ–°èç¸½æ•¸: {len(news_items)}")
        print(f"ğŸ“‚ æª”æ¡ˆå­˜åœ¨: {os.path.exists(filepath)}")
        print(f"ğŸ“¦ æª”æ¡ˆå¤§å°: {os.path.getsize(filepath) if os.path.exists(filepath) else 0} bytes")
        
        # ç”Ÿæˆéƒµä»¶å…§å®¹
        doc_names = [doc.get('name', 'æœªå‘½å') for doc in req.documents]
        email_body = generate_news_report_html(
            document_name=f"æ‰¹æ¬¡åŒ¯å‡ºï¼ˆ{len(req.documents)} å€‹æ–‡ä»¶ï¼‰",
            news_items=news_items
        )
        
        print(f"ğŸ“§ æº–å‚™ç™¼é€éƒµä»¶è‡³: {req.recipient_email}")
        print(f"ğŸ“ é™„ä»¶è·¯å¾‘: {filepath}")
        print(f"ğŸ“ é™„ä»¶åç¨±: {filename}")
        
        # ç™¼é€éƒµä»¶
        email_result = send_email_with_attachment(
            to_email=req.recipient_email,
            subject=req.subject,
            body=email_body,
            attachment_path=filepath,
            attachment_name=filename
        )
        
        print(f"ğŸ“¬ éƒµä»¶ç™¼é€çµæœ: {email_result}")
        
        # æ¸…ç†èˆŠæª”æ¡ˆï¼ˆä¿ç•™ 7 å¤©ï¼‰
        cleanup_old_exports(output_dir=str(output_dir), max_age_days=7)
        
        if email_result.get("success"):
            return JSONResponse(
                content={
                    "success": True,
                    "message": f"å·²æˆåŠŸåŒ¯å‡º {excel_result['count']} ç­†æ–°èä¸¦ç™¼é€è‡³ {req.recipient_email}",
                    "filename": filename,
                    "count": excel_result["count"],
                    "documents_count": len(req.documents)
                }
            )
        else:
            return JSONResponse(
                status_code=500,
                content={
                    "success": False,
                    "error": email_result.get("error", "ç™¼é€éƒµä»¶å¤±æ•—"),
                    "excel_generated": True,
                    "filepath": filepath
                }
            )
    
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"success": False, "error": f"æ‰¹æ¬¡è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}"}
        )


@app.delete("/api/news/records/{record_id}")
async def delete_news_record(record_id: str):
    """
    åˆªé™¤æŒ‡å®šçš„æ–°èè¨˜éŒ„
    """
    try:
        print(f"[DELETE API] æ”¶åˆ°åˆªé™¤è«‹æ±‚: {record_id}")
        print(f"[DELETE API] è³‡æ–™åº«è·¯å¾‘: {news_store.db_path}")
        
        # åˆªé™¤å‰å…ˆæª¢æŸ¥è¨˜éŒ„æ˜¯å¦å­˜åœ¨
        existing = news_store.get_record_by_id(record_id)
        print(f"[DELETE API] åˆªé™¤å‰æª¢æŸ¥è¨˜éŒ„: {existing is not None}")
        
        success = news_store.delete_record(record_id)
        print(f"[DELETE API] åˆªé™¤çµæœ: {success}")
        
        # åˆªé™¤å¾Œå†æ¬¡æª¢æŸ¥
        check_after = news_store.get_record_by_id(record_id)
        print(f"[DELETE API] åˆªé™¤å¾Œæª¢æŸ¥è¨˜éŒ„: {check_after is not None}")
        
        if success:
            return JSONResponse(
                status_code=200,
                content={"success": True, "message": "è¨˜éŒ„å·²åˆªé™¤"}
            )
        else:
            # è¨˜éŒ„ä¸å­˜åœ¨ï¼Œä½†é€™ä¸æ‡‰è©²ç®—éŒ¯èª¤ï¼ˆå†ªç­‰æ€§ï¼‰
            return JSONResponse(
                status_code=200,
                content={"success": True, "message": "è¨˜éŒ„å·²åˆªé™¤æˆ–ä¸å­˜åœ¨"}
            )
    except Exception as e:
        print(f"[ERROR] åˆªé™¤è¨˜éŒ„å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse(
            status_code=500,
            content={"success": False, "error": f"åˆªé™¤è¨˜éŒ„å¤±æ•—: {str(e)}"}
        )


@app.put("/api/news/records/{record_id}/tags")
async def update_news_record_tags(record_id: str, tags: List[str]):
    """
    æ›´æ–°æ–°èè¨˜éŒ„çš„æ¨™ç±¤
    """
    try:
        success = news_store.update_tags(record_id, tags)
        if success:
            return JSONResponse(content={"success": True, "message": "æ¨™ç±¤å·²æ›´æ–°"})
        else:
            return JSONResponse(
                status_code=404,
                content={"success": False, "error": "è¨˜éŒ„ä¸å­˜åœ¨"}
            )
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"success": False, "error": f"æ›´æ–°æ¨™ç±¤å¤±æ•—: {str(e)}"}
        )


@app.post("/api/news/records")
async def save_news_record(record: Dict[str, Any]):
    """
    ä¿å­˜æ–°èè¨˜éŒ„åˆ°æ•¸æ“šåº«
    """
    try:
        success = news_store.add_record(record)
        if success:
            return JSONResponse(content={"success": True, "message": "è¨˜éŒ„å·²ä¿å­˜"})
        else:
            return JSONResponse(
                status_code=500,
                content={"success": False, "error": "ä¿å­˜å¤±æ•—"}
            )
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"success": False, "error": f"ä¿å­˜è¨˜éŒ„å¤±æ•—: {str(e)}"}
        )


# ============ é™æ€æ–‡ä»¶æœåŠ¡é…ç½®ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰ ============
# æ³¨æ„ï¼šéœæ…‹æ–‡ä»¶æœå‹™åœ¨ startup_event() ä¸­é…ç½®
# é€™æ¨£å¯ä»¥ç¢ºä¿åœ¨æ‰€æœ‰ API è·¯ç”±å®šç¾©ä¹‹å¾Œæ‰æ›è¼‰
# ä½¿ç”¨ StaticFiles çš„ html=True åƒæ•¸é¿å… 405 éŒ¯èª¤
