"""
Excel ç”Ÿæˆæœå‹™
å¾æ–°èæ–‡ä»¶å…§å®¹ä¸­è§£ææ–°èåˆ—è¡¨ä¸¦ç”Ÿæˆ Excel å ±å‘Š
"""
import os
import re
from datetime import datetime
from typing import List, Dict, Any, Optional
from openpyxl import Workbook
from openpyxl.styles import Font, Alignment, PatternFill
from openpyxl.utils import get_column_letter
from openai import OpenAI


def extract_country_from_content(content: str, fallback_name: str = "") -> str:
    """
    å¾æ–‡ç« å…§å®¹ä¸­åˆ¤æ–·åœ‹å®¶ï¼ˆä½¿ç”¨è¦å‰‡å¼ URL ç¶²åŸŸåŒ¹é…ï¼Œä¸ä½¿ç”¨ LLMï¼‰
    
    Args:
        content: æ–‡ç« å…§å®¹
        fallback_name: å‚™ç”¨çš„æ–‡ä»¶åç¨±
        
    Returns:
        åœ‹å®¶åç¨±ï¼ˆä¸­æ–‡ï¼‰
    """
    # å€åŸŸè‹±æ–‡è½‰ä¸­æ–‡æ˜ å°„
    region_to_chinese = {
        'Vietnam': 'è¶Šå—',
        'Thailand': 'æ³°åœ‹',
        'Singapore': 'æ–°åŠ å¡',
        'Philippines': 'è²å¾‹è³“',
        'Cambodia': 'æŸ¬åŸ”å¯¨',
        'Indonesia': 'å°å°¼',
        'Malaysia': 'é¦¬ä¾†è¥¿äº',
        'Myanmar': 'ç·¬ç”¸',
        'Laos': 'å¯®åœ‹',
        'Southeast Asia': 'æ±å—äº',
    }
    
    # ç¶²åŸŸåˆ°åœ‹å®¶çš„æ˜ å°„ï¼ˆå¾ TRUSTED_NEWS_SOURCES æå–ï¼‰
    domain_to_country = {
        'viet-jo.com': 'è¶Šå—',
        'cafef.vn': 'è¶Šå—',
        'vnexpress.net': 'è¶Šå—',
        'vietnamfinance.vn': 'è¶Šå—',
        'vir.com.vn': 'è¶Šå—',
        'vietnambiz.vn': 'è¶Šå—',
        'tapchikinhtetaichinh.vn': 'è¶Šå—',
        'bangkokpost.com': 'æ³°åœ‹',
        'techsauce.co': 'æ³°åœ‹',
        'fintechnews.sg': 'æ–°åŠ å¡',
        'fintechnews.ph': 'è²å¾‹è³“',
        'khmertimeskh.com': 'æŸ¬åŸ”å¯¨',
        'cc-times.com': 'æŸ¬åŸ”å¯¨',
        'phnompenhpost.com': 'æŸ¬åŸ”å¯¨',
        'dealstreetasia.com': 'æ±å—äº',
        'techinasia.com': 'æ±å—äº',
        'asia.nikkei.com': 'æ±å—äº',
        'heaptalk.com': 'æ±å—äº',
    }
    
    # å˜—è©¦å¾å…§å®¹ä¸­çš„ URL æå–åœ‹å®¶
    import re
    url_pattern = r'https?://(?:www\.)?([a-zA-Z0-9.-]+)'
    urls = re.findall(url_pattern, content)
    
    for url_domain in urls:
        # æª¢æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•å·²çŸ¥ç¶²åŸŸ
        for domain, country in domain_to_country.items():
            if domain in url_domain:
                return country
    
    # å¦‚æœæ²’æ‰¾åˆ° URLï¼Œä½¿ç”¨æ–‡ä»¶åç¨±åˆ¤æ–·ï¼ˆåŸæœ‰é‚è¼¯ï¼‰
    return extract_country_from_name(fallback_name)


def extract_country_from_name(name: str) -> str:
    """
    å¾æ–‡ä»¶åç¨±ä¸­æå–åœ‹å®¶åç¨±ï¼ˆå‚™ç”¨æ–¹æ³•ï¼‰
    
    Args:
        name: æ–‡ä»¶åç¨±
        
    Returns:
        åœ‹å®¶åç¨±ï¼ˆä¸­æ–‡ï¼‰
    """
    if not name:
        return " "
        
    name_lower = name.lower()
    
    # åœ‹å®¶é—œéµå­—æ˜ å°„
    country_mapping = {
        'è¶Šå—': ['è¶Šå—', 'vietnam', 'vn', 'viet'],
        'æ³°åœ‹': ['æ³°åœ‹', 'thailand', 'thai'],
        'å°å°¼': ['å°å°¼', 'indonesia', 'indonesian'],
        'è²å¾‹è³“': ['è²å¾‹è³“', 'philippines', 'philippine', 'filipino'],
        'æŸ¬åŸ”å¯¨': ['æŸ¬åŸ”å¯¨', 'cambodia', 'cambodian'],
        'æ–°åŠ å¡': ['æ–°åŠ å¡', 'singapore'],
        'é¦¬ä¾†è¥¿äº': ['é¦¬ä¾†è¥¿äº', 'malaysia', 'malaysian'],
        'ç·¬ç”¸': ['ç·¬ç”¸', 'myanmar', 'burma'],
        'å¯®åœ‹': ['å¯®åœ‹', 'laos', 'lao']
    }
    
    # æª¢æŸ¥åç¨±ä¸­æ˜¯å¦åŒ…å«åœ‹å®¶é—œéµå­—
    for country, keywords in country_mapping.items():
        for keyword in keywords:
            if keyword in name_lower:
                return country
    
    # å¦‚æœæ²’æœ‰åŒ¹é…åˆ°ï¼Œè¿”å›æœªçŸ¥
    return " "


def translate_title_to_chinese(title: str) -> str:
    """
    å°‡æ–°èæ¨™é¡Œç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
    
    Args:
        title: åŸå§‹æ¨™é¡Œ
        
    Returns:
        ç¹é«”ä¸­æ–‡æ¨™é¡Œ
    """
    if not title:
        return title
    
    # ç°¡å–®æª¢æ¸¬ï¼šå¦‚æœæ¨™é¡Œä¸­ä¸­æ–‡å­—ç¬¦è¶…é 30%ï¼Œèªç‚ºå·²ç¶“æ˜¯ä¸­æ–‡
    chinese_chars = sum(1 for char in title if '\u4e00' <= char <= '\u9fff')
    if chinese_chars / len(title) > 0.3:
        return title
    
    # ä½¿ç”¨æ‰¹æ¬¡ç¿»è­¯å‡½æ•¸è™•ç†å–®å€‹æ¨™é¡Œ
    results = batch_translate_titles([title])
    return results.get(title, title)


def batch_translate_titles(titles: List[str]) -> Dict[str, str]:
    """
    æ‰¹æ¬¡ç¿»è­¯å¤šå€‹æ–°èæ¨™é¡Œç‚ºç¹é«”ä¸­æ–‡ï¼ˆå–®æ¬¡ API å‘¼å«ï¼‰
    
    Args:
        titles: åŸå§‹æ¨™é¡Œåˆ—è¡¨
        
    Returns:
        å­—å…¸ {åŸå§‹æ¨™é¡Œ: ç¿»è­¯å¾Œæ¨™é¡Œ}
    """
    if not titles:
        return {}
    
    # éæ¿¾å·²ç¶“æ˜¯ä¸­æ–‡çš„æ¨™é¡Œ
    titles_to_translate = []
    result = {}
    
    for title in titles:
        if not title:
            result[title] = title
            continue
        chinese_chars = sum(1 for char in title if '\u4e00' <= char <= '\u9fff')
        if len(title) > 0 and chinese_chars / len(title) > 0.3:
            result[title] = title  # å·²ç¶“æ˜¯ä¸­æ–‡ï¼Œä¸éœ€ç¿»è­¯
        else:
            titles_to_translate.append(title)
    
    # å¦‚æœæ‰€æœ‰æ¨™é¡Œéƒ½æ˜¯ä¸­æ–‡ï¼Œç›´æ¥è¿”å›
    if not titles_to_translate:
        return result
    
    try:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            # æ²’æœ‰ API keyï¼Œè¿”å›åŸå§‹æ¨™é¡Œ
            for title in titles_to_translate:
                result[title] = title
            return result
        
        client = OpenAI(api_key=api_key)
        
        # æ§‹å»ºæ‰¹æ¬¡ç¿»è­¯æç¤º
        numbered_titles = "\n".join([f"{i+1}. {t}" for i, t in enumerate(titles_to_translate)])
        
        prompt = f"""è«‹å°‡ä»¥ä¸‹æ–°èæ¨™é¡Œç¿»è­¯æˆç¹é«”ä¸­æ–‡ã€‚æ¯è¡Œä¸€å€‹æ¨™é¡Œï¼Œä¿æŒç·¨è™Ÿå°æ‡‰ã€‚åªéœ€å›ç­”ç¿»è­¯çµæœï¼Œä¸è¦æœ‰å…¶ä»–èªªæ˜ã€‚

åŸæ¨™é¡Œåˆ—è¡¨ï¼š
{numbered_titles}

ç¹é«”ä¸­æ–‡ç¿»è­¯ï¼ˆä¿æŒç·¨è™Ÿæ ¼å¼ï¼‰ï¼š"""
        
        response = client.chat.completions.create(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_completion_tokens=500
        )
        
        translated_text = response.choices[0].message.content.strip()
        print(f"ğŸ“ æ‰¹æ¬¡ç¿»è­¯å®Œæˆ: {len(titles_to_translate)} å€‹æ¨™é¡Œ")
        
        # è§£æç¿»è­¯çµæœ
        lines = translated_text.split("\n")
        for i, title in enumerate(titles_to_translate):
            if i < len(lines):
                # ç§»é™¤ç·¨è™Ÿå‰ç¶´ (å¦‚ "1. ", "2. " ç­‰)
                translated = re.sub(r'^\d+\.\s*', '', lines[i]).strip()
                result[title] = translated if translated else title
            else:
                result[title] = title
        
        return result
        
    except Exception as e:
        import traceback
        print(f"æ‰¹æ¬¡æ¨™é¡Œç¿»è­¯å¤±æ•—: {e}")
        print(f"éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}")
        # ç¿»è­¯å¤±æ•—æ™‚è¿”å›åŸå§‹æ¨™é¡Œ
        for title in titles_to_translate:
            result[title] = title
        return result


def parse_news_from_content(content: str) -> List[Dict[str, str]]:
    """
    å¾æ–‡ä»¶å…§å®¹ä¸­è§£ææ–°èåˆ—è¡¨
    æ”¯æŒå…©ç¨®æ ¼å¼ï¼š
    1. å–®ç¯‡æ–°èæ–‡æª”ï¼ˆNEWS é¡å‹ï¼‰ï¼šåŒ…å«æ¨™é¡Œã€ç™¼å¸ƒæ™‚é–“ã€å…§å®¹å’Œä¾†æº
    2. å¤šç¯‡æ–°èåˆ—è¡¨ï¼ˆRESEARCH é¡å‹ï¼‰ï¼šMarkdown æ ¼å¼ï¼Œæ¨™é¡Œç‚º ###
    
    Args:
        content: æ–‡ä»¶å…§å®¹ï¼ˆMarkdown æ ¼å¼ï¼‰
        
    Returns:
        æ–°èåˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ åŒ…å« title, date, summary, link
    """
    news_items = []
    original_titles = []  # æ”¶é›†æ‰€æœ‰åŸå§‹æ¨™é¡Œ
    
    # æª¢æ¸¬æ˜¯å¦ç‚ºå–®ç¯‡æ–°èæ–‡æª”ï¼ˆä»¥ # æ¨™é¡Œé–‹é ­ï¼ŒåŒ…å«ç™¼å¸ƒæ™‚é–“å’Œä¾†æºï¼‰
    if content.strip().startswith('# ') and '**ç™¼å¸ƒæ™‚é–“**' in content and '**ä¾†æº**' in content:
        # å–®ç¯‡æ–°èæ ¼å¼
        news_item = {
            'title': '',
            'original_title': '',  # æš«å­˜åŸå§‹æ¨™é¡Œ
            'date': '',
            'summary': '',
            'link': ''
        }
        
        lines = content.strip().split('\n')
        
        # æå–æ¨™é¡Œï¼ˆç¬¬ä¸€è¡Œï¼Œå»æ‰ # ç¬¦è™Ÿï¼‰
        if lines:
            original_title = lines[0].replace('#', '').strip()
            news_item['original_title'] = original_title
            original_titles.append(original_title)
        
        # æå–ç™¼å¸ƒæ™‚é–“
        date_pattern = r'\*\*ç™¼å¸ƒæ™‚é–“\*\*[ï¼š:]\s*(\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}[æ—¥]?)'
        date_match = re.search(date_pattern, content)
        if date_match:
            news_item['date'] = date_match.group(1).replace('å¹´', '-').replace('æœˆ', '-').replace('æ—¥', '')
        
        # æå–ä¾†æº URL
        source_pattern = r'\*\*ä¾†æº\*\*[ï¼š:]\s*(https?://[^\s]+)'
        source_match = re.search(source_pattern, content)
        if source_match:
            news_item['link'] = source_match.group(1).strip()
        
        # æå–æ‘˜è¦ï¼ˆç§»é™¤æ¨™é¡Œã€ç™¼å¸ƒæ™‚é–“å’Œä¾†æºå¾Œçš„å…§å®¹ï¼‰
        summary_text = content
        summary_text = re.sub(r'^#[^\n]+\n+', '', summary_text)  # ç§»é™¤æ¨™é¡Œ
        summary_text = re.sub(r'\*\*ç™¼å¸ƒæ™‚é–“\*\*[ï¼š:][^\n]+\n*', '', summary_text)  # ç§»é™¤ç™¼å¸ƒæ™‚é–“
        summary_text = re.sub(r'ç™¼å¸ƒæ™‚é–“[ï¼š:][^\n]+\n*', '', summary_text)  # ç§»é™¤æ²’æœ‰ç²—é«”çš„ç™¼å¸ƒæ™‚é–“
        summary_text = re.sub(r'\*\*ä¾†æº\*\*[ï¼š:][^\n]+', '', summary_text)  # ç§»é™¤ä¾†æº
        # ç§»é™¤æ‰€æœ‰ URL
        summary_text = re.sub(r'`https?://[^`]+`', '', summary_text)  # ç§»é™¤åå¼•è™Ÿä¸­çš„ URL
        summary_text = re.sub(r'\[[^\]]*\]\([^\)]*https?://[^\)]*\)', '', summary_text)  # ç§»é™¤ Markdown é€£çµ
        summary_text = re.sub(r'https?://[^\s\)\]]+', '', summary_text)  # ç§»é™¤æ‰€æœ‰å…¶ä»– URL
        # ç§»é™¤ç‰¹æ®Šæ¨™è¨˜ç¬¦è™Ÿå’Œæ ¼å¼
        summary_text = re.sub(r'\*\*[^*]+\*\*[ï¼š:]?', '', summary_text)  # ç§»é™¤ç²—é«”æ¨™è¨˜
        summary_text = re.sub(r'[\[\]\(\)ã€Œã€ã€ã€ã€ã€‘]', '', summary_text)  # ç§»é™¤å„ç¨®æ‹¬è™Ÿ
        summary_text = re.sub(r'[â€¢Â·â–ªâ–¸â–ºâ–¶]', '', summary_text)  # ç§»é™¤åˆ—è¡¨ç¬¦è™Ÿ
        # éæ¿¾éä¸­è‹±æ–‡å­—ç¬¦ï¼ˆä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•¸å­—ã€å¸¸ç”¨æ¨™é»ï¼‰
        summary_text = re.sub(r'[^\u4e00-\u9fff\u3000-\u303fa-zA-Z0-9\sï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š,.!?\'\"%-]', '', summary_text)
        summary_text = re.sub(r'\n+', ' ', summary_text)  # åˆä½µæ›è¡Œ
        summary_text = re.sub(r'\s+', ' ', summary_text)  # åˆä½µç©ºç™½
        summary_text = summary_text.strip()
        
        news_item['summary'] = summary_text[:500] if summary_text else ''
        
        if news_item['original_title'] and news_item['summary']:
            news_items.append(news_item)
    else:
        # åŸæœ‰çš„å¤šç¯‡æ–°èåˆ—è¡¨è§£æé‚è¼¯ï¼ˆRESEARCH é¡å‹ï¼‰
        # å…ˆç§»é™¤æ–‡æœ«çš„ç¸½çµå€å¡Š
        summary_section_pattern = r'\n##\s+(æ‘˜è¦|æœŸé–“é‡é»|è¦†è“‹åº¦|ç¼ºå£|å¾ŒçºŒå»ºè­°|Credit Memo).*$'
        content = re.sub(summary_section_pattern, '', content, flags=re.DOTALL)
        
        # æŒ‰ ### æ¨™é¡Œåˆ†å‰²æ–°èé …ç›®
        sections = re.split(r'\n###\s+', content)
        
        for section in sections:
            section = section.strip()
            if not section or len(section) < 20:
                continue
            
            # è·³ééæ–°èæ¨™é¡Œ
            first_line = section.split('\n')[0].strip()
            if any(keyword in first_line for keyword in ['å›è¦†é‡é»', 'è¶Šå—', 'æ³°åœ‹', 'å°å°¼', 'è²å¾‹è³“', 'æŸ¬åŸ”å¯¨', 'Vietnam', 'Thailand', 'Indonesia', 'Philippines', 'Cambodia']):
                continue
            if first_line.startswith('#') or first_line.startswith('ã€'):
                continue
            
            news_item = {
                'title': '',
                'original_title': '',  # æš«å­˜åŸå§‹æ¨™é¡Œ
                'date': '',
                'summary': '',
                'link': ''
            }
            
            # æå–æ¨™é¡Œï¼ˆç¬¬ä¸€è¡Œï¼‰
            lines = section.split('\n')
            if lines:
                original_title = lines[0].strip()
                news_item['original_title'] = original_title
                original_titles.append(original_title)
            
            # æå–ç™¼å¸ƒæ™‚é–“
            date_pattern = r'ç™¼å¸ƒæ™‚é–“[ï¼š:]\s*(\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}[æ—¥]?)'
            date_match = re.search(date_pattern, section)
            if date_match:
                news_item['date'] = date_match.group(1).replace('å¹´', '-').replace('æœˆ', '-').replace('æ—¥', '')
            
            # æå–é€£çµ
            link_pattern = r'`(https?://[^`]+)`'
            link_match = re.search(link_pattern, section)
            if link_match:
                news_item['link'] = link_match.group(1).strip()
            else:
                plain_link_match = re.search(r'(https?://[^\s\)]+)', section)
                if plain_link_match:
                    news_item['link'] = plain_link_match.group(1).strip()
            
            # æå–æ‘˜è¦
            summary_text = section
            if lines:
                summary_text = '\n'.join(lines[1:])
            # ç§»é™¤ç™¼å¸ƒæ™‚é–“ï¼ˆå¤šç¨®æ ¼å¼ï¼‰
            summary_text = re.sub(r'\*\*ç™¼å¸ƒæ™‚é–“\*\*[ï¼š:][^\n]+\n*', '', summary_text)  # ç²—é«”æ ¼å¼
            summary_text = re.sub(r'ç™¼å¸ƒæ™‚é–“[ï¼š:][^\n]+\n*', '', summary_text)  # æ™®é€šæ ¼å¼
            summary_text = re.sub(r'\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}[æ—¥]?', '', summary_text)  # ç§»é™¤æ—¥æœŸæ ¼å¼
            summary_text = re.sub(r'---+.*$', '', summary_text, flags=re.DOTALL)
            # ç§»é™¤æ‰€æœ‰é¡å‹çš„ URL
            summary_text = re.sub(r'\[([^\]]*)\]\([^\)]*\)', r'\1', summary_text)  # Markdown é€£çµè½‰æ–‡å­—
            summary_text = re.sub(r'\([^\)]*https?://[^\)]*\)', '', summary_text)  # æ‹¬è™Ÿå…§çš„ URL
            summary_text = re.sub(r'\[[^\]]*\]', '', summary_text)  # ç§»é™¤å‰©é¤˜çš„ä¸­æ‹¬è™Ÿ
            summary_text = re.sub(r'`https?://[^`]+`', '', summary_text)  # åå¼•è™Ÿä¸­çš„ URL
            summary_text = re.sub(r'https?://[^\s\)\]]+', '', summary_text)  # æ‰€æœ‰å…¶ä»– URL
            summary_text = re.sub(r'#+\s*', '', summary_text)
            summary_text = re.sub(r'\(\s*\)', '', summary_text)  # ç§»é™¤ç©ºæ‹¬è™Ÿ
            # ç§»é™¤ç‰¹æ®Šæ¨™è¨˜ç¬¦è™Ÿ
            summary_text = re.sub(r'\*\*[^*]+\*\*[ï¼š:]?', '', summary_text)  # ç§»é™¤ç²—é«”æ¨™è¨˜
            summary_text = re.sub(r'[â€¢Â·â–ªâ–¸â–ºâ–¶]', '', summary_text)  # ç§»é™¤åˆ—è¡¨ç¬¦è™Ÿ
            # éæ¿¾éä¸­è‹±æ–‡å­—ç¬¦ï¼ˆä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•¸å­—ã€å¸¸ç”¨æ¨™é»ï¼‰
            summary_text = re.sub(r'[^\u4e00-\u9fff\u3000-\u303fa-zA-Z0-9\sï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š,.!?\'\"%-]', '', summary_text)
            summary_text = re.sub(r'\n+', ' ', summary_text)
            summary_text = re.sub(r'\s+', ' ', summary_text)
            summary_text = summary_text.strip()
            
            news_item['summary'] = summary_text[:500] if summary_text else ''
            
            # åªæœ‰æ¨™é¡Œå’Œæ‘˜è¦éƒ½å­˜åœ¨æ™‚æ‰åŠ å…¥åˆ—è¡¨
            if news_item['original_title'] and news_item['summary']:
                news_items.append(news_item)
    
    # æ‰¹æ¬¡ç¿»è­¯æ‰€æœ‰æ¨™é¡Œï¼ˆå–®æ¬¡ API å‘¼å«ï¼‰
    if original_titles:
        title_translations = batch_translate_titles(original_titles)
        for item in news_items:
            original = item.get('original_title', '')
            item['title'] = title_translations.get(original, original)
            del item['original_title']  # ç§»é™¤æš«å­˜æ¬„ä½
    
    return news_items


def generate_news_excel(
    document_name: str,
    document_content: str,
    output_dir: str = "exports"
) -> Dict[str, Any]:
    """
    å¾æ–‡ä»¶å…§å®¹ç”Ÿæˆæ–°è Excel å ±å‘Š
    
    Args:
        document_name: æ–‡ä»¶åç¨±
        document_content: æ–‡ä»¶å…§å®¹ï¼ˆåŒ…å«æ–°èåˆ—è¡¨ï¼‰
        output_dir: è¼¸å‡ºç›®éŒ„
        
    Returns:
        åŒ…å« success, filepath, filename, count, news_items çš„å­—å…¸
    """
    try:
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # è§£ææ–°èåˆ—è¡¨
        news_items = parse_news_from_content(document_content)
        
        if not news_items:
            return {
                "success": False,
                "error": "æœªèƒ½å¾æ–‡ä»¶ä¸­è§£æå‡ºæ–°èé …ç›®"
            }
        
        # å¾æ•¸æ“šåº«è¨˜éŒ„ç²å–åœ‹å®¶ï¼ˆä¸å†é‡è¤‡èª¿ç”¨ LLMï¼‰
        from news_store import news_store
        country = ""  # é»˜èªå€¼æ”¹ç‚ºç©ºå­—ç¬¦ä¸²
        
        # å˜—è©¦é€šéæ–‡ä»¶åæŸ¥è©¢æ•¸æ“šåº«ç²å–åœ‹å®¶
        try:
            all_records = news_store.get_all_records()
            for record in all_records:
                if record.get('name') == document_name:
                    country = record.get('country', '')
                    # å¦‚æœæ˜¯'æœªçŸ¥'ä¹Ÿè¦–ç‚ºç©º
                    if country == ' ':
                        country = ''
                    break
        except Exception as e:
            print(f"âš ï¸ å¾æ•¸æ“šåº«ç²å–åœ‹å®¶å¤±æ•—: {e}")
        
        # å¦‚æœæ•¸æ“šåº«ä¸­æ²’æœ‰æ‰¾åˆ°ï¼Œå˜—è©¦å¾ tags ä¸­ç²å–
        if not country:
            try:
                for record in all_records:
                    if record.get('name') == document_name:
                        tags = record.get('tags', [])
                        if tags and len(tags) > 0:
                            country = tags[0] if tags[0] != ' ' else ''
                        break
            except Exception as e:
                print(f"âš ï¸ å¾ tags ç²å–åœ‹å®¶å¤±æ•—: {e}")
        
        for item in news_items:
            item['source_doc'] = country
        
        # å‰µå»º Excel
        wb = Workbook()
        ws = wb.active
        ws.title = "æ–°èå ±å‘Š"
        
        # è¨­å®šæ¨™é¡Œè¡Œ
        headers = ["ç·¨è™Ÿ", "æ–°èæ¨™é¡Œ", "ç™¼å¸ƒæ™‚é–“", "æ–°èæ‘˜è¦", "æ–°èé€£çµ", "ä¾†æºåœ‹å®¶"]
        ws.append(headers)
        
        # è¨­å®šæ¨™é¡Œæ¨£å¼
        header_fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
        header_font = Font(bold=True, color="FFFFFF", size=11)
        header_alignment = Alignment(horizontal="center", vertical="center")
        
        for col in range(1, len(headers) + 1):
            cell = ws.cell(row=1, column=col)
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = header_alignment
        
        # å¡«å…¥æ–°èè³‡æ–™
        for idx, news in enumerate(news_items, start=1):
            ws.append([
                idx,
                news.get('title', ''),
                news.get('date', ''),
                news.get('summary', ''),
                news.get('link', ''),
                news.get('source_doc', '')
            ])
        
        # è¨­å®šæ¬„å¯¬
        column_widths = {
            'A': 8,   # ç·¨è™Ÿ
            'B': 40,  # æ¨™é¡Œ
            'C': 15,  # æ™‚é–“
            'D': 60,  # æ‘˜è¦
            'E': 50,  # é€£çµ
            'F': 20   # ä¾†æºåœ‹å®¶
        }
        
        for col, width in column_widths.items():
            ws.column_dimensions[col].width = width
        
        # è¨­å®šè³‡æ–™è¡Œæ¨£å¼
        data_alignment = Alignment(vertical="top", wrap_text=True)
        for row in range(2, len(news_items) + 2):
            for col in range(1, len(headers) + 1):
                cell = ws.cell(row=row, column=col)
                cell.alignment = data_alignment
        
        # å‡çµé¦–è¡Œ
        ws.freeze_panes = "A2"
        
        # ç”Ÿæˆæª”æ¡ˆåç¨±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_doc_name = re.sub(r'[^\w\s-]', '', document_name)[:30]
        filename = f"æ–°èå ±å‘Š_{safe_doc_name}_{timestamp}.xlsx"
        filepath = os.path.join(output_dir, filename)
        
        # å„²å­˜æª”æ¡ˆ
        wb.save(filepath)
        
        return {
            "success": True,
            "filepath": filepath,
            "filename": filename,
            "count": len(news_items),
            "news_items": news_items
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"ç”Ÿæˆ Excel æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        }


def cleanup_old_exports(output_dir: str = "exports", max_age_days: int = 7):
    """
    æ¸…ç†è¶…éæŒ‡å®šå¤©æ•¸çš„åŒ¯å‡ºæª”æ¡ˆ
    
    Args:
        output_dir: è¼¸å‡ºç›®éŒ„
        max_age_days: ä¿ç•™å¤©æ•¸
    """
    try:
        if not os.path.exists(output_dir):
            return
        
        current_time = datetime.now()
        max_age_seconds = max_age_days * 24 * 60 * 60
        
        for filename in os.listdir(output_dir):
            filepath = os.path.join(output_dir, filename)
            
            if not os.path.isfile(filepath):
                continue
            
            # æª¢æŸ¥æª”æ¡ˆå¹´é½¡
            file_age = current_time.timestamp() - os.path.getmtime(filepath)
            
            if file_age > max_age_seconds:
                try:
                    os.remove(filepath)
                    print(f"å·²åˆªé™¤èˆŠæª”æ¡ˆ: {filename}")
                except Exception as e:
                    print(f"åˆªé™¤æª”æ¡ˆå¤±æ•— {filename}: {e}")
    
    except Exception as e:
        print(f"æ¸…ç†èˆŠæª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")


def generate_batch_news_excel(
    documents: List[Dict[str, str]],
    output_dir: str = "exports"
) -> Dict[str, Any]:
    """
    æ‰¹æ¬¡åŒ¯å‡ºå¤šå€‹æ–‡ä»¶çš„æ–°èåˆ°ä¸€å€‹ Excel æª”æ¡ˆ
    
    Args:
        documents: æ–‡ä»¶åˆ—è¡¨ï¼Œæ¯å€‹æ–‡ä»¶åŒ…å« name å’Œ content
        output_dir: è¼¸å‡ºç›®éŒ„
        
    Returns:
        åŒ…å« success, filepath, filename, count, news_items çš„å­—å…¸
    """
    try:
        os.makedirs(output_dir, exist_ok=True)
        
        # æ”¶é›†æ‰€æœ‰æ–°èé …ç›®
        all_news_items = []
        
        for doc in documents:
            doc_name = doc.get('name', 'æœªå‘½å')
            doc_content = doc.get('content', '')
            
            if not doc_content:
                continue
            
            # è§£æè©²æ–‡ä»¶çš„æ–°è
            news_items = parse_news_from_content(doc_content)
            
            # ä½¿ç”¨ LLM åˆ¤æ–·åœ‹å®¶
            country = extract_country_from_content(doc_content, fallback_name=doc_name)
            for item in news_items:
                item['source_doc'] = country
            
            all_news_items.extend(news_items)
        
        if not all_news_items:
            return {
                "success": False,
                "error": "æ²’æœ‰æ‰¾åˆ°å¯åŒ¯å‡ºçš„æ–°èé …ç›®"
            }
        
        # å‰µå»º Excel
        wb = Workbook()
        ws = wb.active
        ws.title = "æ–°èå ±å‘Š"
        
        # è¨­å®šæ¨™é¡Œè¡Œ
        headers = ["ç·¨è™Ÿ", "æ–°èæ¨™é¡Œ", "ç™¼å¸ƒæ™‚é–“", "æ–°èæ‘˜è¦", "æ–°èé€£çµ", "ä¾†æºåœ‹å®¶"]
        ws.append(headers)
        
        # è¨­å®šæ¨™é¡Œæ¨£å¼
        header_fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
        header_font = Font(bold=True, color="FFFFFF", size=11)
        header_alignment = Alignment(horizontal="center", vertical="center")
        
        for col in range(1, len(headers) + 1):
            cell = ws.cell(row=1, column=col)
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = header_alignment
        
        # å¡«å…¥æ–°èè³‡æ–™
        for idx, news in enumerate(all_news_items, start=1):
            ws.append([
                idx,
                news.get('title', ''),
                news.get('date', ''),
                news.get('summary', ''),
                news.get('link', ''),
                news.get('source_doc', '')
            ])
        
        # è¨­å®šæ¬„å¯¬
        column_widths = {
            'A': 8,   # ç·¨è™Ÿ
            'B': 40,  # æ¨™é¡Œ
            'C': 15,  # æ™‚é–“
            'D': 60,  # æ‘˜è¦
            'E': 50,  # é€£çµ
            'F': 20   # ä¾†æºåœ‹å®¶
        }
        
        for col, width in column_widths.items():
            ws.column_dimensions[col].width = width
        
        # è¨­å®šè³‡æ–™è¡Œæ¨£å¼
        data_alignment = Alignment(vertical="top", wrap_text=True)
        for row in range(2, len(all_news_items) + 2):
            for col in range(1, len(headers) + 1):
                cell = ws.cell(row=row, column=col)
                cell.alignment = data_alignment
        
        # å‡çµé¦–è¡Œ
        ws.freeze_panes = "A2"
        
        # ç”Ÿæˆæª”æ¡ˆåç¨±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"æ–°èå ±å‘Š_æ‰¹æ¬¡åŒ¯å‡º_{timestamp}.xlsx"
        filepath = os.path.join(output_dir, filename)
        
        # å„²å­˜æª”æ¡ˆ
        wb.save(filepath)
        
        return {
            "success": True,
            "filepath": filepath,
            "filename": filename,
            "count": len(all_news_items),
            "news_items": all_news_items
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"æ‰¹æ¬¡ç”Ÿæˆ Excel æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        }
